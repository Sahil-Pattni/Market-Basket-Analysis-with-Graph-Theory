
\chapter{The \algo\ Algorithm}
In this section, we introduce the main contribution of this paper: the Clustered-Graph Rule Generation (CGRG) algorithm. The \algo\ algorithm derives association rules from a clustered minimum spanning tree. This chapter will focus on the workings of the algorithm.

\section{Data Processing}
\label{sec:algo_data}
Given an itemset $I = \{I_1, I_2,\dots,I_m\}$ and a set of tranactions $T = \{T_1,T_2,\dots\,T_n\}$ such that $T_i \subseteq I$, every transaction $t_i$ will be transformed into a binary purchase vector, as described in Section \ref{sec:binary_purchase_vectors}. An $m \times m$ correlation matrix will then be computed using the Pearson's Correlation Coefficient from the set of binary purchase vectors, composed of the correlation every item has against every other item. A graph $G = (V,E)$ will be constructed from this correlation matrix, such that the vertices $V=I$, and every edge will be the correlation value between the two items it connects. A minimum spanning tree is a sub-graph composed of the \textit{shortest} path connecting all vertices (i.e. the lowest sum of weights), and therefore to do so from our graph, the MST would only capture strongly negative and weakly positive relationships only, whereas we want to capture the strongest relationships between products. Therefore to extract a MST with the strongest associations, the values in the correlation matrix must first be transformed before a graph is constructed from them.

\section{Distance Function}
\label{sec:distance}
As mentioned in Section \ref{sec:binary_purchase_vectors}, the Pearson's Correlation Coefficient is equivalent to the Phi Coefficient ($\phi$)for binary variables, as is the case for our binary purchase vectors. Given $\phi_{ij}$ for items $I_i$ and $I_j$ in the correlation matrix, \pcite{mst_paper} used the distance function
\[
\phi_{ij} = \sqrt{2(1-\phi_{ij})}
\]
to transform their correlation matrix. Given that we are using the Pearson's Correlation Coefficient that gives us a correlation score in the interval [-1,1], this equation is not suitable for us as it discards the strongly negative associations by outputting a larger weight (e.g. a correlation score of -1 would be transformed to 2) which would not be captured in the MST. Therefore, we modify the equation to be
\[
\phi_{ij} = \sqrt{2(1-|\phi_{ij}|)}
\]
allowing us to capture both positive and negative associations alike. Figure \ref{fig:distance_function} illustrates the effect of our distance function, where the stronger the correlation of the input, the lower the output. Given that our initial correlation values were bound to the interval $[-1,1]$, the output of the distance function is similarly bound to the interval $[0, \sqrt{2}]$.
\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{distance_function}
\label{fig:distance_function}
\caption{The effect of the distance function $\sqrt{2(1-|x|)}$}
\end{figure}
\noindent Once the correlation matrix has been transformed via this function, the graph $G=(V,E)$ can be constructed. Using Kruskal's algorithm, the minimum spanning tree can then be constructed from the graph $G$.

\section{Clustering and Rule Generation}
\subsection{Markov Clustering}
Once the MST has been constructed, it can then be clustered using the Markov Clustering (MCL) Algorithm. Multiple cluster configurations will be generated using the algorithm with varying inflation scores to identify the most modular configuration\footnote{Where the connectedness between dense clusters is minimal}, which will then be used to segment the MST.

\subsection{Itemset Generation}
For every cluster $Cl_i$ identified by the MCL algorithm, all proper subsets $cl$ such that $cl \subseteq Cl_i$ are generated. The set of proper subsets are stored against the key $i$, which denotes the index of cluster $Cl_i$, effectively assigning each cluster its proper subsets. Listing \ref{lst:itemset_generation} is the pseudo-code for this operation.
\begin{lstlisting}[language=C, mathescape=true, caption=Cluster Itemset Generation, label=lst:itemset_generation]
given CLUSTERS = {$Cl_1, Cl_2,\dots,Cl_n$}
define itemsets_by_cluster as {$\emptyset$}
for $Cl_i$ in CLUSTERS
    to items_by_cluster add ({all $cl$ if $cl \subseteq Cl_i$} at index $i$)
\end{lstlisting}

\subsection{Bi-Cluster Rule Generation}
Bi-cluster rules are those where the antecedent and the consequent originate from distinct and separate clusters. All possible bi-cluster rules are generated such that for any two given clusters $Cl_k$ and $Cl_j$:
\[
cl_k \rightarrow cl_j;\;\; cl_k \subset Cl_k, \; cl_j \subset Cl_j
\]
Listing \ref{lst:bicluster} presents the pseudo-code for the bi-cluster rule generation. All bi-cluster combinations are generated and stored as a pair of indices (i.e. $(i,j)$ for clusters $Cl_i$ and $Cl_j$). For every cluster pair generated, every configuration itemsets associated with each cluster (calculated in Listing \ref{lst:itemset_generation}) is added to the ruleset such that a rule exists for every itemset $c_k$ associated with cluster $C_k$ and every itemset $c_j$ associated with cluster $C_j$.

\begin{minipage}{\linewidth}
\begin{lstlisting}[language=C, mathescape=true, caption=Bi-Cluster Rule Generation, label=lst:bicluster]
given CLUSTERS = {$Cl_1, Cl_2,\dots,Cl_n$}
given itemsets_by_cluster = {$\dots$} // already populated
define rules as $\emptyset$
define combs as $\emptyset$
to combs add all $^nC_2$ combinations from {$1,2,\dots,n$}

for comb in combs:
    define antecedent_itemsets as (itemsets_by_cluster at (c at index 1)) // first index is 1
    define consequent_itemsets as (itemsets_by_cluster at (c at index 2))
    for antecedent in antecedent_itemsets:
        for consequent_item in consequent:
            to rules add (antecedent $\rightarrow$ consequent)
            to rules add (consequent $\rightarrow$ antecedent)
\end{lstlisting}
\end{minipage}

\subsection{Intra-Cluster Rule Generation}
Intra-cluster rules are those where itemsets in both the antecedent and consequent originate from the same cluster. This can be represented as:
\[
cl_{k1} \rightarrow cl_{k2}; \;\;\;\; cl_{k1} \subseteq Cl_k,\; cl_{k2} \subseteq Cl_k,\; cl_{k1} \cap cl_{k2} = \emptyset
\]

\begin{lstlisting}[language=C, mathescape=true, caption=Intra-Cluster Rule Generation, label=lst:intracluster]
given CLUSTERS = {$Cl_1, Cl_2,\dots,Cl_n$}
given itemsets_by_cluster = {$\dots$} // already populated
define rules as $\emptyset$
for $i$ in itemsets_by_cluster
    define itemsets as (itemsets_by_cluster at index i) // itemsets at $i^{th}$ cluster.
    define combs = $\emptyset$
    to combs add all $^pC_2$ combinations from itemsets // For all p elements in itemsets
    
    for comb in combs
        define antecedent as (comb at index 1) // first index is 1
        define consequent as (comb at index 2)
        to rules add (antecedent $\rightarrow$ consequent)
        to rules add (consequent $\rightarrow$ antecedent) 
\end{lstlisting}
Listing \ref{lst:intracluster} is the pseudo-code for the intra-cluster rule generation. A rule is generated from every configuration of two subsets from the subsets associated with a given cluster.

\subsection{Rule Pruning}
\begin{lstlisting}[language=C, mathescape=true, caption=Rule Pruning, label=lst:prune]
given rules = {$\dots$}                 // already populated
sort rules ascending by antecedent // sort by number of items in antecedent
define min_support as float        // user-specified value
define min_confidence as float     // user-specified value
define valid_rules as $\emptyset$
define below_threshold as $\emptyset$
for r in rules
    antecedent = (r at index 1) // first index is 1
    consequent = (r at index 2)
    define is_above_threshold = True 
    // check if support lookup can be skipped
    for itemset in below_threshold
        if itemset $\subset$ r
            is_above_threshold = false
            break
    if is_above_threshold
        // calculate all supports
        define support_antecedent as float
        define support_consequent as float
        define support_r as float 
        define confidence as (support_r $\div$ support_antecedent)
        define lift as (confidence $\div$ support_consequent)
        if support_antecedent < min_support
            to below_threshold add support_antecedent
        if support_consequent < min_support
            to below_threshold add support_consequent
        if confidence < min_confidence:
            next loop // move to next rule in for loop
        if support_r < min_support
            next loop 
        // If all constraints met, add rule
        to valid_rules add r
\end{lstlisting}
Listing \ref{lst:prune} presents the pseudo-code for rule pruning, employing the use of the \textit{Apriori Principle} described in Section \ref{sec:ais}, which states that \textit{the support of a set is - at most - equal to the support of its subsets}. We first sort the rules in ascending order by the number of items in the antecedent of each rule. When iterating through the rules, if the support of either the antecedent or consequent is found to be below the user specified support constraint, it is added \texttt{below\_threshold} - the set of itemsets found to be below our constraint. For every subsequent iteration, if it is found that any set from \texttt{below\_threshold} is a subset of the current rule, we can discard the rule and avoid having to calculate its support as we know it will be below our support constraint. Our final rules are those that remain after pruning.\\
Figure \ref{fig:algorithm_flow} illustrates the complete flow for the \algo\ algorithm.
\begin{figure}[H]
\centering
\includegraphics[scale=0.35]{ruleflow}
\caption{\algo\ algorithm}
\label{fig:algorithm_flow}
\end{figure}

%\section{Dataset Pre-Processing}
%The dataset used in this study is the sales data of $4,152,919$ transactions from a chain of Brazilian gas-station stores \pcite{data_source}.
%Each row in the dataset represents the purchase of a specific product as part of a transaction - and as such, each row corresponds to the following columns:
%\begin{itemize}
%\item Company Code
%\item Order Number
%\item Employee
%\item Product
%\item Product Category
%\item Client
%\item Sale Date Time
%\item Product Cost
%\item Discount Amount
%\end{itemize}
%All personal and corporate names were exchanged for fictitious names by the author of the dataset in order to preserve the anonymity of those whose who could have otherwise been identified through the dataset.
%% columns_to_export = ['product', 'product_category', 'client_city', 'discount_amount', 'basket_id']
%Only the Product, Product Category, Client City and Discount Amount columns were retained for the purposes of our algorithm, the rest were discarded.
%Before employing the dataset, sanitary procedures were carried out to ensure that the dataset was error-free and in a format suitable for graph generation. The steps have been detailed below.
%
%\begin{enumerate}
%\item \textbf{Transaction Identifier}\\
%The \textit{Order Number} field showed discrepancies, where a given order number could reference distinct transactions in different stores and cities, and at different dates and times. 
%This could be due to the stores maintaining their own order numbers, and also because the order numbers may reset after a predetermined limit.
%A unique transaction identifier - named \texttt{basket\_id} - was created by concatenating the order number and the date, thereby mitigating the occurrence of a identifier that references multiple transactions.
%
%\item \textbf{Binary Purchase Vector transformation}\\
%The dataset was then transformed such that each transaction was represented by a binary purchase vector - as described in Section \ref{sec:binary_purchase_vectors} - wherein each column represents a product category. The product categories were chosen for the graph representation over the products themselves as it would give a more generalized view on the associations between them, and the categories themselves were deemed specific enough that they would not be parent to children of significant variance.
%
%\end{enumerate} 
%\section{MST Generation}
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.5]{category_dist}
%\caption{Category Distribution}
%\label{fig:cat_dist}
%\end{figure}
%The metrics used to assess the association rules - support, lift and confidence - are based on the proportional presence of a given itemset in the transactions. Since our dataset is from a gas-station store chain, fuel products dominate the transactional presence by a significant factor. Figure \ref{fig:cat_dist} highlights the disparity between the presence of \textit{fuel} products and the others, with \textit{fuel} being present in $99.28\%$ of all transactions. To avoid the association rules being dominated by the \textit{fuel} category - which should inherently understood to be a key product for gas stations - the fuel category was purged from the dataset, reducing the dataset to $1,362,617$ transactions.
%
%
%Following the methodology described in Section \ref{sec:binary_purchase_vectors}, a correlation matrix was computed from the binary purchase vectors using the Pearson's Correlation Coefficient. Since the correlations are of binary variables again, the correlation score is equivalent to the Phi-Coefficient $\phi$. Figure \ref{fig:correlation} illustrates the correlation matrix. Only the values below the diagonal have been illustrated as the matrix is diagonally symmetrical.
%
%
%
%
%
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.4]{correlation}
%\caption{Correlation Matrix from Binary Purchase Vectors}
%\label{fig:correlation}
%\end{figure}
%
%A minimum spanning tree is a graph composed of the \textit{shortest} path connecting all vertices, and therefore the values in the correlation matrix would need to be transformed via a function such that the higher the correlation value, the lower the output value. This would allow the MST to capture the strongest associations between the product categories. As used by \textbf{M. A. Valle et al.} in \pcite{mst_paper}, the distance function (Equation \ref{eq:distance_function}  was used to make this transformation - where $\phi_{ij}$ refers to the correlation value for two given product categories $i$ and $j$ in Figure \ref{fig:correlation}). However, whereas \textbf{M. A. Valle et al.} left their $\phi_{ij}$ unaltered, we have converted it to an absolute value (i.e. $|\phi_{ij}|$). The reason behind this is to capture the strongest relationships between product categories, positive and negative alike. By leaving the value unaltered, the MST would not capture strong negative associations as they would have the highest weights after the distance function was applied. Figure \ref{fig:distance_function} illustrates the effect of this function, where the stronger the correlation, the lower the distance function's output. The figure also demonstrates that because the Pearson's Correlation Coefficient is bound to the interval $[-1,1]$, the distance function's output is therefore bound to the interval $[0,\sqrt{2}]$. 
%\begin{equation}
%\label{eq:distance_function}
%d_{ij} = \sqrt{2(1-|\phi_{ij}|)}
%\end{equation}
%
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.4]{distance_function}
%\label{fig:distance_function}
%\caption{The effect of the distance function $\sqrt{2(1-|x|)}$}
%\end{figure}
%
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.32]{graph_and_mst_no_fuel}
%\caption{Product Category Graph and MST}
%\label{fig:graph_mst}
%\end{figure}
%With the correlation matrix transformed using Equation \ref{eq:distance_function}, a graph $G = (V,E)$ was generated such that the vertices $V$ represent the product categories, and the edges $E$ the associations between them. Employing Kruskal's algorithm, a minimum spanning tree was then extracted from this graph. Both the complete graph and the MST are illustrated in Figure \ref{fig:graph_mst}. The value of each node is an integer, which corresponds with the index of the product category in the binary purchase vector dataset. The length of each edge is directly proportionate to its value, such that the greater the value, the greater the length of the edge.
%
%
%
%\section{Markov Clustering}
%The MST was then clustered using the Markov Clustering algorithm. To identify the most modular clustering configuration (modularity being the minimality of connectedness between dense clusters), an array of inflation scores between $1.5$ and $2.5$ (inclusive) were tested at increments of $0.1$, leading us to determine that an inflation score of $1.6$ resulted in the most optimal modularity. The Markov Clustering was performed using this inflation score, and the clustering results are illustrated in Figure \ref{fig:clustered} (note that while the disposition of the nodes is different from that illustrated in Figure \ref{fig:graph_mst}, the values of all nodes and edges are the same). 
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.32]{mst_clustered_no_fuel2}
%\caption{MST before and after Markov Clustering}
%\label{fig:clustered}
%\end{figure}
%The Markov Clustering algorithm segmented the nodes into 8 distinct clusters.
%Figure \ref{fig:cluster_named} illustrates the names of the product categories in each cluster, color-coded in accordance with the graphs in Figure \ref{fig:clustered}.
%
%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.3]{cluster_named}
%\caption{Product categories by cluster}
%\label{fig:cluster_named}
%\end{figure}
%
%\section{Rule Generation}
%
%\subsection{Itemset Generation}
%For every cluster identified, all possible itemsets are generated for the $n$ product categories in a given cluster such that the length of the itemsets range from $1$ to $n-1$. Listing \ref{lst:itemset_generation} is the pseudo-code that generates all possible itemsets for a cluster.
%
%%\begin{lstlisting}[language=Python, caption=Cluster Itemset Generation, label=lst:itemset_generation]
%%# Source: arm_cython.pyx
%%from itertools import combinations
%%...
%%cpdef __generate_itemsets_by_cluster():
%%    global itemsets_by_cluster
%%    cdef set items
%%    for index, cluster in enumerate(clusters):
%%        items = set()
%%        for set_size in range(1, len(cluster)):
%%            items.update(combinations(cluster, set_size))
%%        itemsets_by_cluster[index] = items
%%\end{lstlisting}
%
%
%\begin{lstlisting}[language=C, mathescape=true, caption=Cluster Itemset Generation, label=lst:itemset_generation]
%define itemsets_by_cluster as {$\emptyset$}
%define CLUSTERS as {$c_1, c_2,\dots,c_n$}
%function generate_itemsets_by_cluster
%	for i in {$1,2,\dots,n$}
%        define items as $\emptyset$
%        // Cluster at index i
%        define cluster as CLUSTERS at i
%        for j in {$1,2,\dots,n-1$}
%            to items add all $^nC_j$ combinations of j elements from cluster
%        // Add items to items_by_cluster with cluster index as key
%        // e.g. {2: {(1,4),(2)...}}
%        to items_by_cluster add cluster_index at items
%\end{lstlisting}
%Once the minimum spanning tree has been successfully clustered, association rules can be generated from the clusters in two distinct ways.
%\subsection{Bi-Cluster Rule Generation}
%Bi-cluster rules are those where the antecedent and consequent originate from distinct and separate clusters. All possible bi-cluster permutations $^nP_2$ are calculated for $n$ clusters. Possible association rules are then generated for these combinations of clusters.
%\begin{lstlisting}[language=C, mathescape=true, caption=Cluster Itemset Generation, label=lst:bi_cluster]
%define CLUSTERS as {$c_1, c_2,\dots,c_n$}
%define itemsets_by_cluster as {$\dots$} // populated above
%function generate_bicluster_rules
%    define rules as {}
%    // all bi-cluster combinations
%    define combinations as all $^nC_2$ combinations of {$1,2,\dots,n$} with 2 elements
%    
%    for c in combinations:
%        define antecedent_cluster as (c at 1) // first index is 1
%        define consequent_cluster as (c at 2)
%        define antecedent as (itemsets_by_cluster at antecedent_cluster)
%        define consequent as (itemsets_by_cluster at consequent_cluster)
%        for antecedent_item in antededent
%            for consequent_item in consequent
%                to rules add (antecedent_item $\rightarrow$ consequent_item)
%                to rules add (consequent_item $\rightarrow$ antecedent_item)
%\end{lstlisting}
%Listing \ref{lst:bi_cluster} is the pseudo-code used to generate all the bi-cluster rules. In the pseudo-code, we have used combinations instead of permutations to gather all bi-cluster configurations. 
%If we were to use permutations, we would have to generate each itemset $i$ for clusters $A$ and $B$ such that
%	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset A,\;\; i_j \subset B\;\; i_k \cap i_j = \emptyset\]
%and then
%	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset B,\;\; i_j \subset A\;\; i_k \cap i_j = \emptyset\]
%which would be the same combinations in a different configuration. Instead, by using \texttt{combinations} on line $4$ of Listing \ref{lst:bi_cluster}, only computing the $i_k \rightarrow i_j$ and then re-adding the rule to the ruleset with the antecedent and consequent swapped (line $12$), we reduces our computational time for the operation by half.
%
%\subsection{Intra-Cluster Rule Generation}
%Intra-cluster rules are those where the both antecedent and consequent originate from the same cluster. Similar to the bi-cluster rules, the intra-cluster rules used the itemsets generated via the \texttt{\_\_generate\_itemsets\_by\_cluster()} function in Listing \ref{lst:itemset_generation}, however with the constraint:
%	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset A,\;\; i_j \subset A\;\; i_k \cap i_j = \emptyset\]
%
%\subsection{Rule Pruning}
%\begin{lstlisting}[language=C, mathescape=true, caption=Rule Prune, label=lst:prune]
%define rules as {$\dots$} // all possible association rules
%define min_support as float
%define min_confidence as float
%function prune_rules
%    define valid_rules as {$\emptyset$}
%    define below_thresold as {$\emptyset$}
%    sort rules ascending by antecedent
%    for r in rules
%        // de-structure rule
%        antecedent = r at 1 // index begins at 1
%        consequent = r at 2
%        define is_above_threshold as true
%        for itemset in below_threshold
%            if itemset $\subset$ r
%                is_above_threshold = false
%                break
%        if is_above_threshold
%            // calculate supports
%            define support_antecedent as float
%            define support_consequent as float
%            define support_r as float
%            define confidence as (support_r $\div$ support_antecedent)
%            define lift as (confidence $\div$ support_consequent)
%            
%            if support_antecedent < min_support
%                to below_threshold add support_antecedent
%                next loop
%            if support_consequent < min_support
%                to below_threshold add support_consequent
%                next loop
%            if confidence < min_confidence
%                next loop
%            
%            to valid_rules add r
%\end{lstlisting}
%As described in section \ref{sec:ais}, \textit{The Apriori Principle} states that the support of a set is - at most - equal to the support of its subsets.
%\[\text{support}(\{x,y\}) \geq \text{support}(\{x,y,z\})\]
%To take advantage of this, we first sort our rules in ascending order by the number of elements in the antecedent. When iterating through the potential rules, if either the antecedent or consequent is found to have a support score below the pre-defined minimum tolerance, the itemsets are added to a set \texttt{below\_threshold}. Any future rules iterated are then cross-checked against \texttt{below\_threshold}, 
%and if it is found that any set in \texttt{below\_threshold} is a subsset of the rule, the rule is discarded as we know that its support is below our minimum tolerance. This allows us to avoid computing the support for several rules - the time complexity of which $O(n)$ per rule at worst, thereby significantly reducing computation time. For both the \textit{bi-cluster} and \textit{intra-cluster} rule generation, the rules are pruned accordingly, and those that satisfy the thresholds specified are returned.

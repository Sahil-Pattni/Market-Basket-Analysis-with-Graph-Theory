\chapter{Background}

\section{Transaction and Association Representation}
\subsection{Binary Purchase Vectors}
\label{sec:binary_purchase_vectors}
Let $I = \{I_1, I_2,\dots,I_m\}$ be a set of binary attributes (i.e. items), and let $T$ be a database of transactions.
As defined in \pcite{mine}, a binary purchase vector is a transaction $t$ represented as a vector of length $m$, where:
\[
t[k] = 
\begin{cases}
1 & \text{if } I_k \text{ purchased in } t\\
0 & \text{otherwise}\\
\end{cases}
\]
For example, consider a grocer who only sells five items: milk, eggs, bread, apples and oranges.
Consider the following transactions:
\begin{description}
\item[Transaction $t_1$:] Customer purchases bread and oranges.
\item[Transcation $t_2$:] Customer purchases eggs, bread and apples.
\item[Transaction $t_3$:] Customer purchases milk and oranges.
\end{description}
The binary purchase vectors for these transactions would be:
\begin{table}[H]
\centering
\begin{tabular}{@{}cccccc@{}}

\toprule
 & milk & eggs & bread & apples & oranges \\ \midrule
$t_1$ &  0    & 0    & 1     & 0      & 1       \\ \bottomrule
$t_2$ &  0    & 1    & 1     & 1      & 0       \\ \bottomrule
$t_3$ &  1    & 0    & 0     & 0      & 1       \\ \bottomrule
\end{tabular}
\caption{Binary Purchase Vectors}
\label{tab:binary_vectors}
\end{table}
\subsection{Product Association}
The association between products pairs across all transactions can be ascertained from the binary purchase vectors using the Pearson's Correlation Coefficient \pcite{pearson}. Since the correlations are of two binary variables, the Pearson's Correlation Coefficient is equivalent to the Phi-Coefficient $\phi$ \pcite{phi}, where for $n$ observations:
\[
\phi = \sqrt{\frac{\chi^2}{n}}
\]
Applying this correlation formula to the set of binary purchase vectors in Table \ref{tab:binary_vectors}, we get:
\begin{table}[H]
\centering
\begin{tabular}{@{}r|ccccc@{}}
\toprule
 & milk & eggs & bread & apples & oranges \\ \midrule
milk &   1.0    & -0.5    & -1.0     & -0.5      & 0.5       \\ \bottomrule
eggs &   -0.5    & 1.0    & 0.5     & 1.0      & -1.0       \\ \bottomrule
bread &  -1.0    & 0.5    & 1.0     & 0.5      & -0.5       \\ \bottomrule
apples & -0.5    & 1.0    & 0.5     & 1.0     & -1.0      \\ \bottomrule
oranges& 0.5    & -1.0    & -0.5     & -1.0      & 1.0      \\ \bottomrule
\end{tabular}
\caption{Correlation Matrix}
\label{tab:correlation}
\end{table}
\noindent Note that the correlation matrix is diagonally symmetrical (i.e. $\phi_{ij} = \phi_{ji}$). This is true of all correlation matrices where the items on the x-axis and y-axis are the same and in the same order. This correlation matrix can then be represented as a graph, where the nodes are the products, and the edges are the correlation values.

\section{Graph Theory}
\begin{figure}[H]
\centering
\includegraphics[scale=1.5]{graph-example}
\caption{Undirected and Directed Graphs}
\label{fig:graph.example}
\end{figure}
In discrete mathematics and more specifically - graph theory, a graph is a data structure that contains a set of nodes (i.e. vertices) connected by lines (i.e. edges).  These edges may be undirected - such as in Figure \ref{fig:graph.example}:(a), or directed - such as in Figure \ref{fig:graph.example}:(b). The edges contain values (i.e. weights) between the two vertices it connects, and the weight of an edge is sometimes represented visually by the edge's length. A graph $G$ with a set of vertices $V$ and a set of edges $E$ can be represented via the notation $G = (V,E)$. For the scope of this project,  we will be building undirected weighted graphs, where there is a singular weight between any two vertices.

\subsection{Minimum Spanning Trees}
Given an undirected $G = (V,E)$,  a \textit{spanning tree} can be described as a subgraph that is a tree which includes all the vertices $V$ of $G$ with the minimum number of edges required. A \textit{minimum spanning tree} (MST) is the spanning tree with the smallest sum of edge weights.  This means that if the graph has $n$ vertices, each spanning tree - including the minimum spanning tree - will have $n-1$ edges. Since a minimum spanning tree captures the lowest weights in a graph, with modifications it could be an excellent candidate to capture the opposite as well: the strongest associations between products.
There are two widely used algorithms to extract the minimum spanning tree from a graph: Prim's algorithm and Kruskal's algorithm.

\subsection{Prim's Algorithm}
Independently discovered by three authors,  Prim's algorithm \pcite{prims}\pcite{prims_og}\pcite{prims3} is a greedy algorithm\footnote{Selecting the locally optimal choice at each iteration of the solution} to find the minimum spanning tree of an undirected,  weighted graph $G$. To successfully implement the algorithm, three sets need to be maintained: a set of \textit{discovered} edges, a set of \textit{undiscovered} vertices, and a set of \textit{discovered} vertices.  Figure \ref{fig:prim} illustrates Prim's algorithm being applied to a graph. The algorithm is as follows:\\
Initialize an empty set of discovered edges: $E$, and two sets of vertices: an empty set $D$ of the discovered vertices, and $UD$ as the set of undiscovered vertices.
\begin{itemize}
\item Pick an arbitrary vertex as a starting point (in the case of Figure \ref{fig:prim}, the top right node). Add this vertex to $D$ and remove it from $UD$.

\item While $UD$ is not empty:
	\begin{itemize}
	\item Find the edge $E_{ij}$ with the smallest weight such that it connects together a vertex $V_i$ in $D$ and $V_j$ in $UD$ (to avoid forming cycles).
	\item Append $V_j$ to $D$ and remove it from $UD$ (i.e. $V_j$ is now discovered).
	\item Append $E_{ij}$ to $E$.
	\end{itemize}
\end{itemize}
Once $D$ contains all the vertices of $G$, the algorithm terminates, and the set $D$ represents the minimum spanning tree, and $\sum{E}$ is the weight of the MST. The time complexity of this algorithm is $O(V^2)$.
\begin{figure}[H]
\centering\includegraphics[scale=0.5]{prims}
\caption{Prim's Algorithm applied to a graph \pcite{prim-pic} }\label{fig:prim}
\end{figure}

\subsection{Kruskal's Algorithm}
Another greedy algorithm,  Kruskal's algorithm \pcite{kruskal} also extracts the MST from a graph.  Unlike Prim's algorithm, Kruskal's doesn't select an edge that connects directly to the already built spanning tree, but rather picks the global optimal solution. The algorithm is as follows:\\
Maintaining a set of edges $E$, and an initially empty set of chosen edges $C$:
\begin{itemize}
\item Sort the set of edges $E$ in ascending order.
\item While the number of elements in $C$ is not $n-1$:
	\begin{itemize}
	\item Select the smallest edge $E_i$ from $E$.
	\item If adding it does not form a cycle with the spanning tree formed so far, append $EE_i$ to $C$.
	\item Remove $E_i$ from $E$.
	\end{itemize}
\end{itemize}
The algorithm will terminate when $n-1$ edges have been selected.  The time complexity for this algorithm is $O(ElogE)$.





\section{Related Work}

\subsection{Extracting Minimum Spanning Trees using K-Means}
\pcite{kmeans_mst} proposed a novel framework to extract the minimum spanning tree of a graph based on the K-Means clustering algorithm. Their methodology can be separated into two distinct phases. In the first phase,  the data is partitioned into $\sqrt{n}$ clusters via K-Means and the Kruskal's algorithm is applied to each of the clusters individually.  Once the $\sqrt{n}$ MSTs have been constructed, they are combined to form an approximate MST. In the second phase,  new partitions are constructed based on the borders of the clusters identified in the first phase.  Based on these new partitions, a second approximate MST is constructed.  Finally, both graphs are merged such that the resulting graph has $2(n-1)$ edges. The Kruskal's algorithm is run on this graph to get the final approximation of the MST.
\\\\\textbf{Critical Analysis}\\
The authors have proposed an efficient way to approximate a minimum spanning tree, with their methodology having a complexity of $O(N^{1.5})$, which is faster than Kruskal's algorithm which has a complexity of $O(N^2)$. 
For clarity, we have illustrated the disparity between the author's algorithm and the standard algorithm on Figure \ref{fig:speed-compare}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{speed-compare} 
\caption{Efficiency of K-Means optimized MST vs.  exact MST}\label{fig:speed-compare}
\end{figure}
\noindent Based on the given time complexities, we have calculated that the proposed K-Means optimized algorithm is $\frac{\sqrt{n}-1}{\sqrt{n}}\%$  faster than Kruskal's algorithm. This algorithm would be optimal for very large graphs where deriving the MST using traditional algorithms such as Prim's or Kruskal's may be inefficient.


\subsection{Markov Clustering}
\pcite{markov_clustering} introduced the novel Markov Clustering Algorithm (MCL), a graph clustering algorithm based on Markov Processes \pcite{markov_original} that favours sparse graphs (i.e. graphs where the average degree is smaller than the number of nodes). 
The reason for this follows the notion that dense regions in sparse graphs correspond with regions in which the number of $k$-length paths is relatively large - and therefore - paths whose beginnings and ends are in the same dense region have a higher probability for random walks\footnote{A random walk is a stochastic process of of successively random steps along a space, in this case: a directed graph.} of length $k$ as opposed to other paths.
In other words, random walks originating from a node in a dense region have a high probability of ending in that same dense region.
Given a non-negative weighted directional graph $G$, the MCL simulates flow in the graph. 
It does this by first mapping the graph $G$ in a generic way onto a Markov matrix $M_1$.
Once completed, the set of transition probabilities (TP's) are iteratively recomputed via expansion and contraction, resulting in an array of Markov Graphs.
In the expansion stage, higher TP's are calculated, whereas for the contraction phase, a new Markov Graph is created by rewarding high TP's and penalizing low TP's.
The belief is that by doing so, the flow between dense regions that are sparsely connected will be removed, leaving only distinct and unconnected dense regions (i.e. clusters).
The author tested the MCL algorithm against randomly generated graphs which were known to possess a natural cluster structure. The authors noted that the MCL algorithm successfully managed to capture the segregation present in the graphs, and concluded that the algorithm was capable of handling graphs with large natural clusters.
\\\\\textbf{Critical Analysis}\\
The author proposed a novel way to apply the mathematics behind Markov processes to successfully segment and isolate densely connected segments of a graph. The results from this paper have inspired us to use the clustering algorithm in our own work.


\subsection{AIS Algorithm}
\label{sec:ais}
\pcite{mine} proposed a novel algorithm to generate all statistically significant association rules between items in a database, laying the foundation for association rule mining. Given a set of items $I = \{I_1, I_2, I_3,\dots,I_m\}$, the authors define an association rule to be of the form $X \rightarrow I_j$ where X is a set of items such that $X \subseteq I,\;\; I_j \notin X$.  The hypothetical database stated was a list of transactions $T$, where each transaction $t$ was a binary vector of length $m$, as described in Section \ref{sec:binary_purchase_vectors}. The authors define two constraints for assessing association rules:
\\\\\textbf{Support}\\
The support of an association rule is the proportion of transactions in which the itemsets in the rule are present. For a set of transactions $T$, where $T(i)$ denotes the set of transactions in which the set of items $i \subset I$ was purchased:
\[
\text{support}(i) = \frac{T(i)}{T} 
\]
Similarly, where $T{(i_k, i_j)}$ represents the set of transactions in which the itemsets $\{i_k \subseteq I, i_j\subseteq I\}$ were purchased, the support for the association rule $i_k \rightarrow i_j$ can be represented as:
\[
\text{support}(i_k \rightarrow i_j) = \frac{T(i_k, i_j)}{T} 
\]
Support scores correspond with the statistical significance of a rule, and rules with low support scores may not occur frequently enough to draw reasonable conclusions from.
\\\\\textbf{Confidence}\\
Confidence is the conditional probability of an itemset $i_j;\;\;i_j \subseteq I$ being present in a transaction given that itemset $i_k;\;\;i_k \subseteq I$ is present in the same transaction. The confidence of the association rule $i_k \rightarrow i_j$ can be represented as:
\[
\text{confidence}(i_k \rightarrow i_j) = \frac{T(i_k, i_j)}{T(i_k)} \equiv \frac{\text{support}(i_k \rightarrow i_j)}{\text{support}(i_k)}
\]
The confidence of an association rule can be thought of as the rule's \textit{strength}.
\\With these constraints defined, the authors state that their methodology for association rule mining can be split into two discrete steps:
\begin{enumerate}
\item The generation of candidate itemsets.
\item The generation of statistically significant association rules from the itemsets.
\end{enumerate}
\textbf{Candidate Itemset Generation}\\
To generate candidate itemsets, the authors first generate all possible itemsets from the database, defining those whose support score was above a support constraint $\textit{min}_\textit{support}$ as \textit{large itemsets}. The authors note that a brute-force check\footnote{checking every possible itemset iteratively.} would be sub-optimal, taking up to $2^m$ passes of the database (where $m$ is the number of items in the itemset $I$). Therefore, they introduced a methodology where they would only observe itemsets of length $k$ on the $k^{th}$ pass of the dataset, to see if the itemsets satisfied the support constraint. On the $(k+1)^{th}$ pass of the dataset, they need only check itemsets that are \textit{1-extensions} (i.e. itemsets extended by only one item) or the \textit{large itemsets} discovered in the previous pass. Their reasoning is now commonly known as \textit{The Apriori Principle}, where they state that if an itemset $i_k$ is \textit{large} (i.e. satisfies the support constraint), then any subset $i_j \subseteq i_k$ will also be \textit{large}. This reasoning also implies that if an itemset $i_j$ is found to be \textit{small} (i.e. did not satisfy the support constraint), then any superset $i_k;\;\; i_j \subseteq i_k$ will also be \textit{small}. This allows the authors to prune the number of association rules whose support scores need to be computed, as if they find $i_j$ to be \textit{small}, any superset $i_k;\;\; i_j \subseteq i_k$ need not have its support score computed as it is known to be \textit{small}.\\
However, if an itemset $i_j$ is indeed found to be \textit{large}, then multiple further passes over the dataset will be required to check the support scores for subsets of $i_j$. To avoid this, the authors devised a measure to calculate the expected support $\bar{s}$ of an itemset. The expected support is used to estimate the support of $i_j = (i_p + i_q)$, not only when $i_j$ is expected to be \textit{large}, but also when $i_p$ is expected to be \textit{large} yet $(i_p + i_q)$ is expected to be \textit{small}. This estimation further prunes the number of rules whose support scores need to be computed.
\\\\\textbf{Association Rule Generation}\\
To generate association rules, the authors used the following technique:\\
for each \textit{large} itemset $Y = \{i_1, i_2,\dots,i_k\};\;\; k \geq 2$ from the set of non-pruned \textit{large} itemsets, generate a set of association rules in the form $X \rightarrow i_j;\;\; X \subseteq Y, i_j \notin X$ such that $X$ is of length $k-1$. Therefore, each \textit{large} itemset will produce $k$ rules. From the generated rules, the authors discarded those rules whose confidence scores fell below the confidence constraint $\textit{min}_\textit{confidence}$.
\\\\\textbf{Evaluation}\\
The authors tested their methodology on a sales dataset with $46,783$ transactions, with $63$ distinct departments. Their configuration was composed of a support constraint of $1\%$ (i.e. $\textit{min}_\textit{support} = 0.01$) and a confidence constraint of $50\%$ (i.e. $\textit{min}_\textit{confidence} = 0.5$). The authors note that the rules produced follow what general intuition might suggest. For example:
\[
\{\text{Auto Accessories, Tires}\} \rightarrow \{\text{Automotive Services}\}
\]
Furthermore, the authors assessed the accuracy of their support estimation metric $\bar{s}$ by observing the ratio of correctly estimated itemsets for both \textit{small} and \textit{large} against various values for the support constraint. They were able to conclude that their estimation accuracy was satisfactory, as their accuracy was  $96\%$ and above for support thresholds.
\\\\\textbf{Critical Analysis}\\
The authors have proposed a novel methodology that has been the bedrock of numerous research publications,  including most of the papers in this literature review.  Their estimation function performed with high accuracy, meaning it can reduce the number of passes through a database the algorithm has to take by a significant amount. Additionally, their pruning techniques allowed them to eliminate a large proportion of itemsets from the space. Even after the significant pruning of rules, a major drawback of this methodology is the large number of rules produced, although one could argue that only the highest performing rules need be observed in further detail. Finally, the algorithm only allows the consequent to have one item, thereby limiting the type and quality of rules produced.

\subsection{Apriori Algorithm}
\label{sec:apriori}
\pcite{apriori} improved on their previous work with \pcite{mine} by introducing the Apriori algorithm, which - in addition to being faster than the AIS algorithm - can produce association rules where the consequent has more than one item. The structure for the Apriori algorithm follows closely to to the AIS algorithm in that it uses \textit{large} itemsets to generate the association rules.
The algorithm generates candidate itemsets in the $k^{th}$ pass only from the itemsets found to be \textit{large} in the $(k-1)^{th}$ pass, following the intuition of \textit{The Apriori Principle}, where any subset of a \textit{large} itemset must itself be \textit{large}. As a result, the candidate itemsets that have $k$ items can be generated from the \textit{large} itemsets having $(k-1)$ itemsets, and any such itemsets that contain a subset that is \textit{small} are discarded.
For every \textit{large} itemset $l$, all non-empty subsets of $l$ are gathered. For every subset $a$, it generates a rule in the form:
\[ a \rightarrow (l-a) \]
if the support and confidence constraints of the rule are met.
\\\\\textbf{Critical Analysis}\\
The authors have further improved upon their AIS algorithm in the form of the Apriori Algorithm, which is much better known than the former due to its inherent ability to generate more complex rules, and its efficiency in doing so. The ability to generate more complex rules makes it a good benchmark to test our own algorithm against, to see whether it can either serve as an alternative or even a complement to the Apriori Algorithm.


\subsection{Subjective Measurement of Association Rules}
\label{sec:subjective}
\pcite{market_ass} proposed a novel measure for the \textit{interestingness} of association rules,  identifying that a dominant, universally used measure did not exist. The authors' goal was to combine objective measures such as the support, confidence and lift scores with more subjective measures. Instead of the Apriori approach, their methodology has them generate association rules via the \textit{tree-building technique} - which compresses a large database into a Frequent-Pattern tree, citing that this technique was more efficient than the Apriori algorithm. The authors employed the heuristical unexpectedness measure\footnote{How significantly a rule contradicts a user's prior beliefs.} and the heuristical actionability measure\footnote{If the user believes they can use the information to their advantage (e.g. a promotion).} as their subjective measures, and a minimum confidence threshold of 51\% as their objective measure.  Since a subjective measure would require a human subject, the authors' used the estimations of a sales manager from a Croatian retail chain, and stored his responses in binary format for the subjective measures (i.e. 0 if a rule was unexpected else 1, 0 if a rule is not useful else 1). The dataset used for this paper was a real transactional dataset with 14,012 transactions and a set of 1,230 unique items (which was later pruned to 7,006 transactions and a set of 278 products) from the same Croatian retail chain their test subject worked at. The authors then generated association rules from the first-level hierarchical grouping of items from the dataset (items with a minimum support of 25\%),  of which 36 rules were identified as statistically significant.  From this set of rules, only two rules satisfied both subjective measures and the confidence constraint, and therefore these two rules were identified as highly interesting.  The authors then generated association rules from the second-level hierarchical grouping of items, where items that represented the same product (but had different a manufacturer, brand etc.) were grouped together. Of the rules generated, 15 satisfied their confidence constraints and had a support value able 10\%. 5 rules from this set satisfied both their subjective and objective measures,  more than the previous experiment  The authors were able to conclude that the increase in accuracy and number of interesting rules resulted from the second level of grouping which generalized the products.\\
\\\\\textbf{Critical Analysis}\\
Wheras the original measure of statistical significance introduced by \pcite{mine} was purely objective, the authors  of this paper have presented a well thought out approach to combining the subjective metrics with objective ones to produce a human-verified association rule set.  A few caveats to note, however: their study only involved one subject, which is rarely regarded to be statistically acceptable. An ideal study would require multiple, randomly chosen subjects to offset any bias that the singular subject would have had, and in addition, the larger their subject size, the closer their collective estimations will model the total population's.  Another drawback of their approach is that by using human intuition as a metric,  they're promoting association rules that satisfy pre-existing notions about human behavior (e.g. if someone buys milk, they'll \textit{probably} get eggs too), however these types of rules are usually regarded as common knowledge,  whereas the utility of association rule mining is in its ability to surface association rules that - while true - seem unintuitive, and therefore are less likely to be known by the management of such organizations.


% MAIN MST PAPER
\subsection{Association Rules from Minimum Spanning Trees}
\label{sec:mst paper}
\pcite{mst_paper} proposed a novel methodology to study the structure and behavior of consumer market baskets from the topology of a minimum spanning tree which represented the interdependencies between products, and use this information to complement the association rule generation process. The input to their proposed methodology was a correlation matrix between the set of all binary purchase vectors for every transaction.  The dataset used for the MST construction was a list of $1,046,804$ transactions containing a set of $3,240$ unique products from a large supermarket chain branch in Santiago, Chile.  When building this correlation matrix, the authors opted to use the Pearson's Coefficient \pcite{pearson} - which is equivalent to the Phi Coefficient $\phi$ for binary data \pcite{phi} - over the traditionally used Jaccard distance to compute the similarity between the binary product vectors, as the former provides both a positive and negative association between products. Additionally,  they used the distance function $d_{ij} = \sqrt{2(1-\phi_{ij})}$ to transform the correlation matrix into a distance measurement (i.e. the weight of the edges)\footnote{$\phi_{ij}$ is the correlation score between products at indexes $i$ and $j$ on the correlation matrix.}. 
\\\\\textbf{MST Analysis}\\
The authors constructed a MST for 220 product subcategories, and noted that there was a significant level of grouping between product sub-categories that belonged to the same parent category.  To remove edges from the MST that were not statistically significant,  the authors used the mutual information measure \pcite{measure} $\sum\limits_{x,y}log_2 \frac{r(x,y)}{p(x)q(y)}$ between product subcategories $p$ and $q$, and were able to prune 14 edges, all of which were connected to a terminal node, therefore effectively pruning 14 vertices from the MST too. To identify the most influential regions of the MST, the authors defined an influence zone of distances that were in the $10^{th}$ percentile. To generate meaningful association rules,  for each MST product $i$, the authors ran a search for the set of all association rules $R_i$ such that $P_i \rightarrow P_j (i \neq j)$. Then from the resulting set of rules, they searched for rules that obeyed $P_i \rightarrow P_m$ where $m$ a product node connected to the product $i$ in the minimum spanning tree.  For both resulting sets of rules for each product, the mean of their lift scores were observed, and the authors determined that the rules that were reinforced by the MST had a higher mean, and that a majority of these rules had a lift score above the $90^{th}$ percentile. 
\\\\\textbf{Inherent Clustering}\\
To identify the clusters each of the products should be identified under, the authors constructed a hierarchical tree using the average linkage clustering method, and by using an unspecified cut distance, they were able to produce 17 taxonomic groups (i.e. clusters). Cross-referencing their results with the actual parent categories of the products, they were able to conclude that the MST did indeed categorize the product sub-categories into clusters with a reasonable degree of accuracy.
\\\\\textbf{Comparing to an alternative methodology}\\
The authors then compared their MST to another methodology, namely the structured association map (SAM) \pcite{kim}, using the Jaccard distance as a measure of similarity,  and were able to generate interesting 2x2 rules (i.e. $\{A,B\}\rightarrow\{C, D\}$), all with lift scores above 1.0, with one rule even having a lift score of 106.46. They concluded that while both approaches provided different information, they both visually identify the strongest relationships between the products, and provide useful information to reduce the search space for association rules.
\\\\\textbf{Critical Analysis}\\
The authors' approach seems to be novel,  thorough and well structured.  Their methodology successfully employed the use of minimum spanning trees to complement the association rule generation process with sound results.  One caveat of their approach is that they only used the MST to generate single-element rules (i.e. where the antecedent and consequent contain only one element each).  While single-element rules are easily understandable and tend to have high lift values when extracted from the MST, multi-element rules would provide an additional layer of insight as to how a range of products (perhaps a cluster) relate to another.


\subsection{Summary}
In conclusion, \pcite{mine} introduced a formal system for association rules, and a method to generate them from a transactional database, which was further improved when \pcite{apriori} introduced the Apriori Algorithm.
Instrumental to the success of this algorithm is its ability to prune a bulk of the rules such that their support scores need not be checked. \pcite{num_rules} defines the equation for calculating the number of possible rules for an itemset \textbf{\textit{d}} as:
\[
\textit{number of rules} = \sum\limits_{k=1}^{d-1} \left(\binom{d}{k} \times \sum\limits_{j=1}^{d-k}\binom{d-k}{j}  \right)
\]
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{numrules}
\caption{Number of Association Rules for an Itemset}
\label{fig:numrules}
\end{figure}
We have plotted Figure \ref{fig:numrules} to illustrate the exponential growth of rules given an itemset's length \textbf{\textit{d}}.
This exponential growth reinforces the need for the rule pruning techniques introduced in \pcite{mine} to be able to complete the computations within a reasonable time frame, and will therefore be incorporated into our methodology.
A drawback of the above methods, however, is the large number of rules produced. Additionally, while these papers only based the interestingness of an association rule on objective measures such as the support and confidence, \pcite{market_ass} proposed a methodology where subjective human input was used to validate the interestingness of these rules. Unfortunately, due to the COVID-19 pandemic at the time of writing, incorporating human subjects into our study was ultimately deemed socially irresponsible, and therefore was not pursued.
\pcite{kmeans_mst} proposed a solution to the relatively slow computation time that the Prims's algorithm and Kruskal's algorithm offer, where the Kruskal's algorithm was optimized using K-Means, leading to a significant performance increase as illustrated in Figure \ref{fig:speed-compare}. The methodology proposed in this paper was a framework, where Kruskal's Algorithm could be substituted with any MST algorithm, such as Prim's Algorithm. While the proposed algorithm would present significant benefits when applied to an extremely large graph, for smaller graphs the reduction in computation time might only be marginal.
\pcite{markov_clustering} introduced a novel way to identify and isolate dense regions within a graph, and we have used their algorithm to cluster our own graphs.
\pcite{mst_paper} introduced a methodology to extract high value association rules from a minimum spanning tree, used to complement the rules produced by the Apriori algorithm. A caveat of this approach, however, is that it can only produce rules such that the antecedent and consequent are sets with one element each. This paper has been the primary motivation for our work, and therefore we have incorporated into our work several techniques that the authors used and/or introduced.
\\As a note, \pcite{lift} defined another metric to assess association rules - \textit{conviction} - now commonly referred to as \textit{lift}. The lift of an association rule $i_k \rightarrow i_j$ is the rise that $i_k$ gives to the confidence of the rule. The formula for lift is:
\[
\text{lift}(i_k \rightarrow i_j) = \frac{\text{confidence}(i_k \rightarrow i_j)}{\text{support}(i_j)}
\] 
All the research conducted above has served as the inspiration for this project. We will be implementing an affinity graph from transactional data in a fashion inspired by \pcite{mst_paper}, and will be using the algorithm proposed by \pcite{kruskal} to derive a minimum spanning tree from the affinity graph. We will also be using the metrics such as support and confidence described in \pcite{mine} and rule pruning techniques similar to those they have mentioned. Lastly, our algorithm will be benchmarked against the Apriori algorithm \pcite{apriori} to evaluate its usefulness.


\chapter{The \algo}

\section{Dataset Pre-Processing}
The dataset used in this study is the sales data of $4,152,919$ transactions from a chain of Brazilian gas-station stores \pcite{data_source}.
Each row in the dataset represents the purchase of a specific product as part of a transaction - and as such, each row corresponds to the following columns:
\begin{itemize}
\item Company Code
\item Order Number
\item Employee
\item Product
\item Product Category
\item Client
\item Sale Date Time
\item Product Cost
\item Discount Amount
\end{itemize}
All personal and corporate names were exchanged for fictitious names by the author of the dataset in order to preserve the anonymity of those whose who could have otherwise been identified through the dataset.
% columns_to_export = ['product', 'product_category', 'client_city', 'discount_amount', 'basket_id']
Only the Product, Product Category, Client City and Discount Amount columns were retained for the purposes of our algorithm, the rest were discarded.
Before employing the dataset, sanitary procedures were carried out to ensure that the dataset was error-free and in a format suitable for graph generation. The steps have been detailed below.

\begin{enumerate}
\item \textbf{Transaction Identifier}\\
The \textit{Order Number} field showed discrepancies, where a given order number could reference distinct transactions in different stores and cities, and at different dates and times. 
This could be due to the stores maintaining their own order numbers, and also because the order numbers may reset after a predetermined limit.
A unique transaction identifier - named \texttt{basket\_id} - was created by concatenating the order number and the date, thereby mitigating the occurrence of a identifier that references multiple transactions.

\item \textbf{Binary Purchase Vector transformation}\\
The dataset was then transformed such that each transaction was represented by a binary purchase vector - as described in Section \ref{sec:binary_purchase_vectors} - wherein each column represents a product category. The product categories were chosen for the graph representation over the products themselves as it would give a more generalized view on the associations between them, and the categories themselves were deemed specific enough that they would not be parent to children of significant variance.

\end{enumerate} 
\section{MST Generation}
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{category_dist}
\caption{Category Distribution}
\label{fig:cat_dist}
\end{figure}
The metrics used to assess the association rules - support, lift and confidence - are based on the proportional presence of a given itemset in the transactions. Since our dataset is from a gas-station store chain, fuel products dominate the transactional presence by a significant factor. Figure \ref{fig:cat_dist} highlights the disparity between the presence of \textit{fuel} products and the others, with \textit{fuel} being present in $99.28\%$ of all transactions. To avoid the association rules being dominated by the \textit{fuel} category - which should inherently understood to be a key product for gas stations - the fuel category was purged from the dataset, reducing the dataset to $1,362,617$ transactions.


Following the methodology described in Section \ref{sec:binary_purchase_vectors}, a correlation matrix was computed from the binary purchase vectors using the Pearson's Correlation Coefficient. Since the correlations are of binary variables again, the correlation score is equivalent to the Phi-Coefficient $\phi$. Figure \ref{fig:correlation} illustrates the correlation matrix. Only the values below the diagonal have been illustrated as the matrix is diagonally symmetrical.





\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{correlation}
\caption{Correlation Matrix from Binary Purchase Vectors}
\label{fig:correlation}
\end{figure}

A minimum spanning tree is a graph composed of the \textit{shortest} path connecting all vertices, and therefore the values in the correlation matrix would need to be transformed via a function such that the higher the correlation value, the lower the output value. This would allow the MST to capture the strongest associations between the product categories. As used by \textbf{M. A. Valle et al.} in \pcite{mst_paper}, the distance function (Equation \ref{eq:distance_function}  was used to make this transformation - where $\phi_{ij}$ refers to the correlation value for two given product categories $i$ and $j$ in Figure \ref{fig:correlation}). However, whereas \textbf{M. A. Valle et al.} left their $\phi_{ij}$ unaltered, we have converted it to an absolute value (i.e. $|\phi_{ij}|$). The reason behind this is to capture the strongest relationships between product categories, positive and negative alike. By leaving the value unaltered, the MST would not capture strong negative associations as they would have the highest weights after the distance function was applied. Figure \ref{fig:distance_function} illustrates the effect of this function, where the stronger the correlation, the lower the distance function's output. The figure also demonstrates that because the Pearson's Correlation Coefficient is bound to the interval $[-1,1]$, the distance function's output is therefore bound to the interval $[0,\sqrt{2}]$. 
\begin{equation}
\label{eq:distance_function}
d_{ij} = \sqrt{2(1-|\phi_{ij}|)}
\end{equation}

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{distance_function}
\label{fig:distance_function}
\caption{The effect of the distance function $\sqrt{2(1-|x|)}$}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.32]{graph_and_mst_no_fuel}
\caption{Product Category Graph and MST}
\label{fig:graph_mst}
\end{figure}
With the correlation matrix transformed using Equation \ref{eq:distance_function}, a graph $G = (V,E)$ was generated such that the vertices $V$ represent the product categories, and the edges $E$ the associations between them. Employing Kruskal's algorithm, a minimum spanning tree was then extracted from this graph. Both the complete graph and the MST are illustrated in Figure \ref{fig:graph_mst}. The value of each node is an integer, which corresponds with the index of the product category in the binary purchase vector dataset. The length of each edge is directly proportionate to its value, such that the greater the value, the greater the length of the edge.



\section{Markov Clustering}
The MST was then clustered using the Markov Clustering algorithm. To identify the most modular clustering configuration (modularity being the minimality of connectedness between dense clusters), an array of inflation scores between $1.5$ and $2.5$ (inclusive) were tested at increments of $0.1$, leading us to determine that an inflation score of $1.6$ resulted in the most optimal modularity. The Markov Clustering was performed using this inflation score, and the clustering results are illustrated in Figure \ref{fig:clustered} (note that while the disposition of the nodes is different from that illustrated in Figure \ref{fig:graph_mst}, the values of all nodes and edges are the same). 
\begin{figure}[H]
\centering
\includegraphics[scale=0.32]{mst_clustered_no_fuel2}
\caption{MST before and after Markov Clustering}
\label{fig:clustered}
\end{figure}
The Markov Clustering algorithm segmented the nodes into 8 distinct clusters.
Figure \ref{fig:cluster_named} illustrates the names of the product categories in each cluster, color-coded in accordance with the graphs in Figure \ref{fig:clustered}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.3]{cluster_named}
\caption{Product categories by cluster}
\label{fig:cluster_named}
\end{figure}

\section{Rule Generation}

\subsection{Itemset Generation}
For every cluster identified, all possible itemsets are generated for the $n$ product categories in a given cluster such that the length of the itemsets range from $1$ to $n-1$. Listing \ref{lst:itemset_generation} is the pseudo-code that generates all possible itemsets for a cluster.

%\begin{lstlisting}[language=Python, caption=Cluster Itemset Generation, label=lst:itemset_generation]
%# Source: arm_cython.pyx
%from itertools import combinations
%...
%cpdef __generate_itemsets_by_cluster():
%    global itemsets_by_cluster
%    cdef set items
%    for index, cluster in enumerate(clusters):
%        items = set()
%        for set_size in range(1, len(cluster)):
%            items.update(combinations(cluster, set_size))
%        itemsets_by_cluster[index] = items
%\end{lstlisting}


\begin{lstlisting}[language=C, mathescape=true, caption=Cluster Itemset Generation, label=lst:itemset_generation]
define itemsets_by_cluster as {$\emptyset$}
define CLUSTERS as {$c_1, c_2,\dots,c_n$}
function generate_itemsets_by_cluster
	for i in {$1,2,\dots,n$}
        define items as $\emptyset$
        // Cluster at index i
        define cluster as CLUSTERS at i
        for j in {$1,2,\dots,n-1$}
            to items add all $^nC_j$ combinations of j elements from cluster
        // Add items to items_by_cluster with cluster index as key
        // e.g. {2: {(1,4),(2)...}}
        to items_by_cluster add cluster_index at items
\end{lstlisting}
Once the minimum spanning tree has been successfully clustered, association rules can be generated from the clusters in two distinct ways.
\subsection{Bi-Cluster Rule Generation}
Bi-cluster rules are those where the antecedent and consequent originate from distinct and separate clusters. All possible bi-cluster permutations $^nP_2$ are calculated for $n$ clusters. Possible association rules are then generated for these combinations of clusters.
\begin{lstlisting}[language=C, mathescape=true, caption=Cluster Itemset Generation, label=lst:bi_cluster]
define CLUSTERS as {$c_1, c_2,\dots,c_n$}
define itemsets_by_cluster as {$\dots$} // populated above
function generate_bicluster_rules
    define rules as {}
    // all bi-cluster combinations
    define combinations as all $^nC_2$ combinations of {$1,2,\dots,n$} with 2 elements
    
    for c in combinations:
        define antecedent_cluster as (c at 1) // first index is 1
        define consequent_cluster as (c at 2)
        define antecedent as (itemsets_by_cluster at antecedent_cluster)
        define consequent as (itemsets_by_cluster at consequent_cluster)
        for antecedent_item in antededent
            for consequent_item in consequent
                to rules add (antecedent_item $\rightarrow$ consequent_item)
                to rules add (consequent_item $\rightarrow$ antecedent_item)
\end{lstlisting}
Listing \ref{lst:bi_cluster} is the pseudo-code used to generate all the bi-cluster rules. In the pseudo-code, we have used combinations instead of permutations to gather all bi-cluster configurations. 
If we were to use permutations, we would have to generate each itemset $i$ for clusters $A$ and $B$ such that
	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset A,\;\; i_j \subset B\;\; i_k \cap i_j = \emptyset\]
and then
	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset B,\;\; i_j \subset A\;\; i_k \cap i_j = \emptyset\]
which would be the same combinations in a different configuration. Instead, by using \texttt{combinations} on line $4$ of Listing \ref{lst:bi_cluster}, only computing the $i_k \rightarrow i_j$ and then re-adding the rule to the ruleset with the antecedent and consequent swapped (line $12$), we reduces our computational time for the operation by half.

\subsection{Intra-Cluster Rule Generation}
Intra-cluster rules are those where the both antecedent and consequent originate from the same cluster. Similar to the bi-cluster rules, the intra-cluster rules used the itemsets generated via the \texttt{\_\_generate\_itemsets\_by\_cluster()} function in Listing \ref{lst:itemset_generation}, however with the constraint:
	\[i_j \rightarrow i_k\;\;\;;\;\;i_k \subset A,\;\; i_j \subset A\;\; i_k \cap i_j = \emptyset\]

\subsection{Rule Pruning}
\begin{lstlisting}[language=C, mathescape=true, caption=Rule Prune, label=lst:prune]
define rules as {$\dots$} // all possible association rules
define min_support as float
define min_confidence as float
function prune_rules
    define valid_rules as {$\emptyset$}
    define below_thresold as {$\emptyset$}
    sort rules ascending by antecedent
    for r in rules
        // de-structure rule
        antecedent = r at 1 // index begins at 1
        consequent = r at 2
        define is_above_threshold as true
        for itemset in below_threshold
            if itemset $\subset$ r
                is_above_threshold = false
                break
        if is_above_threshold
            // calculate supports
            define support_antecedent as float
            define support_consequent as float
            define support_r as float
            define confidence as (support_r $\div$ support_antecedent)
            define lift as (confidence $\div$ support_consequent)
            
            if support_antecedent < min_support
                to below_threshold add support_antecedent
                next loop
            if support_consequent < min_support
                to below_threshold add support_consequent
                next loop
            if confidence < min_confidence
                next loop
            
            to valid_rules add r
\end{lstlisting}
As described in section \ref{sec:ais}, \textit{The Apriori Principle} states that the support of a set is - at most - equal to the support of its subsets.
\[\text{support}(\{x,y\}) \geq \text{support}(\{x,y,z\})\]
To take advantage of this, we first sort our rules in ascending order by the number of elements in the antecedent. When iterating through the potential rules, if either the antecedent or consequent is found to have a support score below the pre-defined minimum tolerance, the itemsets are added to a set \texttt{below\_threshold}. Any future rules iterated are then cross-checked against \texttt{below\_threshold}, 
and if it is found that any set in \texttt{below\_threshold} is a subsset of the rule, the rule is discarded as we know that its support is below our minimum tolerance. This allows us to avoid computing the support for several rules - the time complexity of which $O(n)$ per rule at worst, thereby significantly reducing computation time. For both the \textit{bi-cluster} and \textit{intra-cluster} rule generation, the rules are pruned accordingly, and those that satisfy the thresholds specified are returned.

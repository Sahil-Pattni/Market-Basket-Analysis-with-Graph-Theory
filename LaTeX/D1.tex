\documentclass[a4paper,11pt]{article}

% Padding Requirements
\usepackage[a4paper, margin=1in]{geometry}
% used for double spacing
\usepackage{setspace}
\doublespacing
% Used for images
\usepackage{graphicx}
\usepackage{float}
\graphicspath{{/Users/sloth_mini/Documents/4.1/dissertation/images/}}
% Used for bibliography
\usepackage[style=ieee, sorting=none, backend=biber]{biblatex}
\renewcommand{\bibfont}{\small}
% For math notation
\usepackage{amsmath}
\newcommand{\setA}{\{A\}}
\newcommand{\setB}{\{B\}}
\newcommand{\abrule}{\setA\rightarrow\setB}
% Used for table formatting
\usepackage{booktabs}
\usepackage[table,xcdraw]{xcolor}
\usepackage{longtable}

\addbibresource{references.bib}

% Do we need a table of contents? I don't think so.
\begin{document}
\title{Deliverable 1: Final Year Dissertation}
\author{
	Sahil Pattni\\ 
	BSc. Computer Science (Honours)\\\\
	Supervisor: Neamat El Gayar
	}
\date{\today}
\maketitle
\pagenumbering{arabic}
\newpage
\section*{Declaration} 
I, Sahil Manojkumar Pattni, confirm that this work submitted for assessment is my own and is expressed in my own words. Any uses made within it of the works of other authors in any form (e.g., ideas, equations, figures, text, tables, programs) are properly acknowledged at any point of their use. A list of the references employed is included. 
\\
Signed: Sahil Manojkumar Pattni\\
Date: \today
\newpage
\section*{Abstract}
In this digital age, data is being generated at an exponential rate, with data analytics being used by corporations and small businesses alike to produce insights, reduce costs, optimize operations and increase profits.  Generating association rules allows us to find non-intuitive associations that bring new insights to the management, allowing them to leverage this information to maximize their profits. A prime example would be the \textit{'Beers and Diapers'} urban legend, where a company looked at their point-of-sale data and found a strong association between beers and diapers being co-purchased, which seems rather unintuitive.\\
In this study, a minimum spanning tree (MST) will be extracted from a data set using machine learning techniques. The author will then use this minimum spanning tree to segment products together,  and produce association rules and extract the most interesting ones. The resulting ruleset will be compared to rules generated by the established association rule mining techniques.  Additionally, the author will study how the grouping of the MST compares to clustering algorithms, and how the structure of the MST differs when the dataset is limited to a given city (as opposed to the overall MST), leading to insights that may help the management in such firms make better informed decisions about the type of promotions they would like to run, and the most optimal locations for such.

\newpage
\tableofcontents

\newpage
\section{Introduction}
\subsection{Context}
For businesses which deal with the sale of a heterogeneous physical assets - such as groceries, hypermarkets and select retail outlets - operations such as inventory management and product placement play an instrumental role in determining the business' financial success. These involve asking questions such as:
\begin{itemize}
\item Which products should be placed at the entrance of the store? Which should be placed closer to the exit?
\item Which products will benefit the most by being placed at eye-level?
\item Which products should be placed next to each other to maximize the purchase volume?
\end{itemize}
One way to find optimal solutions for such questions is to employ the use of Association Rule Mining (also known as Market Basket Analysis). This set of techniques assess frequent itemsets (e.g. from sales data) and generate association rules between products. Several algorithms and techniques exist for association rule mining, such as the Apriori Algorithm \cite{apriori} and FP-Growth \cite{fp_growth}. One problem with these algorithms is that they tend to generate an enormous amount of rules, of which many of the rules themselves are large. This makes it grossly inconvenient for the end-user to retrieve any actionable information from the results.\\
The author construct a network of products - where each vertice represents a product or product category, and an edge between two vertices represents the simultaneous occurrence of the the two products. The result is a graph (i.e. a network), whose architecture is visually informative of the products and their associations.  From this graph, a minimum spanning tree (MST) can be extracted, which will prune the graph down to only the strongest connections. The insights that can be gained from the MST can help the management of the mentioned businesses to maximize their profit.  Furthermore,  association rules can now be extracted from the MST.

\subsection{Aims}
The aim of this study is to study the effectiveness of a minimum spanning tree as a market basket analysis tool.  Additionally, this project will study how the architecture of the minimum spanning tree changes when the dataset is limited to a certain city. The model will be built from a large transactional database.

\subsection{Objectives}
The research objectives for this project have been laid out below, in the order that they will be carried out.
\begin{enumerate}
\item Acquire a suitable dataset upon which the MST can be constructed.

\item \label{optimal} Explore and evaluate MST extraction algorithms.

\item Generate an undirected graph $G(E,V)$ where the edges $E$ are the correlation values between the product vertices.

\item Extract a minimum spanning tree from this graph using the technique determined in step \ref{optimal}. 

\item Analyze this MST and use its architecture to determine product clusters and generate association rules.

\item Generate association rules using the Apriori and FP-Growth algorithm and compare them in both their time complexity and their \textit{'interestingness'} (i.e. the unintuitive rules they generate).

\item Validate the product grouping present in the MST against parent categories (if present), and a clustering algorithm (e.g.  such as the K-Means algorithm).
\end{enumerate}

\newpage
\section{Background}
\subsection{Core Concepts} % TODO: Maybe change title?
\subsubsection*{Graphs}
\begin{figure}[H]
\centering
\includegraphics[scale=1.5]{graph-example}
\caption{Undirected and Directed Graphs}
\label{fig:graph.example}
\end{figure}
In discrete mathematics and more specifically - graph theory, a graph is a data structure that contains a set of nodes (i.e. vertices) connected by lines (i.e. edges).  These edges may be directed - such as in Figure \ref{fig:graph.example}:(a), or undirected - such as in Figure \ref{fig:graph.example}:(b). A graph $G$ with a set of vertices $V$ and a set of edges $E$ can be represented via the notation $G = (V,E)$. For the scope of this project,  the author will be building undirected graphs, where the weight between two vertices is the same in both directions.
\subsubsection{Minimum Spanning Trees}
Given an undirected $G = (V,E)$,  a \textit{spanning tree} can be described as a subgraph that is a tree which includes all the vertices $V$ of $G$ with the minimum number of edges required. A \textit{minimum spanning tree} (MST) is the spanning tree with the smallest sum of edge weights.  This means that if the graph has $n$ vertices, each spanning tree - including the minimum spanning tree - will have $n-1$ edges. There are two widely used algorithms to extract the minimum spanning tree from a graph: Prim's algorithm and Kruskal's algorithm.
\subsubsection{Prim's Algorithm}
Independently discovered by three authors,  Prim's algorithm \cite{prims}\cite{prims_og}\cite{prims3} is a greedy algorithm\footnote{Selecting the locally optimal choice at each iteration of the solution} to find the minimum spanning three of an undirected,  weighted graph $G$. To successfully implement the algorithm, three sets need to maintained: a set of \textit{discovered} edges, and two sets of vertices: a set of \textit{undiscovered} vertices, and a set of \textit{discovered} vertices.  Figure \ref{fig:prim} illustrates Prim's algorithm being applied to a graph. The algorithm is as follows:\\
Initialize an empty set of discovered edges: $E$, and two sets of vertices: an empty set $D$ of the discovered vertices, and $UD$ as the set of undiscovered vertices.
\begin{itemize}
\item Pick an arbitrary vertex as a starting point (in the case of Figure \ref{fig:prim}, the top right node). Add this vertex to $D$ and remove it from $UD$.
\item While $UD$ is not empty:
	\begin{itemize}
	\item Find the edge $e_i$ with the smallest weight such that it connects together a vertex $V_1$ in $D$ and $V_2$ in $UD$ (to avoid forming cycles).
	\item Append $V_2$ to $D$ and remove it from $UD$ (i.e. $V_2$ is now discovered).
	\item Append $e_i$ to $E$.
	\end{itemize}
\end{itemize}
Once $D$ contains all the vertices of $G$, the algorithm terminates, and the set $D$ represents the minimum spanning tree, and $\sum\limits_{i=1}^n e_i$ is the weight of the MST. The time complexity of this algorithm is $O(V^2)$.
\begin{figure}[H]
\centering\includegraphics[scale=0.5]{prims}
\caption{Prim's Algorithm applied to a graph \cite{prim-pic} }\label{fig:prim}
\end{figure}

\subsubsection{Kruskal's Algorithm}
Another greedy algorithm,  Kruskal's algorithm \cite{kruskal} also extracts the MST from a graph.  Unlike Prim's algorithm, Kruskal's doesn't select an edge that connects directly to the already build spanning tree, but rather picks the global optimal solution. The algorithm is as follows:\\
Maintaining a set of edges $E$, and an initially empty set of chosen edges $C$:
\begin{itemize}
\item Sort the set of edges $E$ in ascending order.
\item While the number of elements in $C$ is not $n-1$:
	\begin{itemize}
	\item Select the smallest edge $e_i$ from $E$.
	\item If adding it does not form a cycle with the spanning tree formed so far, append $e_i$ to $C$.
	\item Remove $e_i$ from $E$.
	\end{itemize}
\end{itemize}
The algorithm will terminate when $n-1$ edges have been selected.  The time complexity for this algorithm is $O(ElogE)$.
\subsubsection{Clustering and K-Means}
Illustrated in Figure \ref{fig:clustering-eg}, clustering is the act of segregating data into a number of groups (i.e. clusters) such that a data point in a cluster is similar to other data points in that cluster.
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{cluster-eg2}
\caption{An example of clustering}\label{fig:clustering-eg}
\end{figure}
\noindent K-Means Clustering \cite{kmeans} is an unsupervised machine learning algorithm where the data is partitioned into $k$ clusters, where each observation belongs to the cluster with the nearest mean (i.e. centroid).
\subsubsection{Binary Purchase Vectors}
Binary purchase vectors play an instrumental role in the construction of the MST, helping us determine the association between products. What is a binary purchase vector, however? It is a vector whose length is the number of unique products in the dataset, where each element is a 1 or 0 depending on whether the product was purchasd or not. Imagine a grocer who only sells five items: milk, eggs, bread, apples and oranges. If a customer purchases bread and oranges, the binary purchase vector for that transaction would look like this:
\begin{table}[H]
\centering
\begin{tabular}{@{}ccccc@{}}
\toprule
milk & eggs & bread & apples & oranges \\ \midrule
0    & 0    & 1     & 0      & 1       \\ \bottomrule
\end{tabular}
\end{table}
\subsubsection{Market Basket Analysis and Apriori Rule}
\label{mba_define}
Market Basket Analysis (MBA), also known as Affinity Analysis, is a data mining and analysis technique that identifies co-occurrence patterns between products purchased together, and produces association rules for these products as such: $\abrule$ which implies a strong relationship between the purchase of product $A$ and product $B$. In the case of the \textit{beers and pampers} example, the association rule could be represented as $\{Beers\} \rightarrow \{Pampers\}$. The contents of a purchase basket (i.e. the contents of a customer's basket when they check out) is called a \textit{itemset}, which - as the name suggests - is a set of all the items in the basket.  For example, if a customer had bought detergent, bread and soda, the itemset would be $\{\textit{bread, detergent, soda}\}$. Association rules are generated by looking at different combinations of the itemset (e.g. $\{\textit{bread, soda}\}\rightarrow\{\textit{detergent}\}$ and $\{\textit{soda}\}\rightarrow\{\textit{bread}\}$). 
The equation \cite{num_rules} to calculate the number of rules for an itemset of length $d$ is as follows :\\
\begin{equation}
\textit{number of rules} = \sum\limits_{k=1}^{d-1} \left(\binom{d}{k} \times \sum\limits_{j=1}^{d-k}\binom{d-k}{j}  \right)
\label{eq:numrules}
\end{equation}
Analyzing the equation, it becomes apparent that the problem with association rule generation from itemsets is that the number of rules produced grows exponentially with the size of the itemset. To illustrate this, the author has plotted Equation \ref{eq:numrules} in Figure \ref{fig:numrules} for values ranging from 2 to 14.
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{numrules}
\caption{Number of Association Rules for an Itemset}
\label{fig:numrules}
\end{figure}
Clearly, checking all itemsets in a database would be computationally expensive, and so only the \textit{frequent itemsets} - itemsets that have a support value above a minimum threshold $\textit{min}_{\textit{support}}$ - will be considered. Even so, checking each itemset's support score via brute force is unacceptably time consuming, therefore the search for frequent itemsets can be optimized using the Apriori algorithm \cite{apriori}, which states that \textit{all subsets of a frequent itemset must be frequent}. To understand how the Apriori principle can be applied here,  the following three metrics have to be considered:
\\\textbf{Support}\\
The support of a set of products is the fraction of transactions in which these set of products are present. For example,  for a list of transactions $T$, for the product $X$ where $T(X)$ denotes that the set of transactions in which X was present:
\[
\textit{support}(\{A\}) = \frac{T(A)}{T}
\]
and similarly, 
\[
\textit{support}(\abrule) = \frac{T(A,B)}{T}
\]
denotes the support for the rule $\abrule$. Rules with a low support score can be pruned as they indicate a rule does not occur enough to draw any reasonable conclusions from.
\\\textbf{Confidence}\\
Confidence is the measure of how likely a product will be in a basket given that another product is in it. That is to say,  the confidence of a rule $\abrule$ is the conditional probability that $\setB$ occurs in the basket given that $\setA$ is present. The confidence of a rule can be denoted as:
\[
\textit{confidence}(\abrule) = \frac{T(A, B)}{T(A)} \equiv \frac{\textit{support}(\abrule)}{\textit{support}(\setA)}
\]
\\\textbf{Lift}\\
The lift of a rule $\abrule$ is the rise (i.e. \textbf{lift}) that $\setA$ gives to the $\textit{confidence}(\abrule)$.
\[
\textit{lift}(\abrule) = \frac{confidence(\abrule)}{support(\setB)}
\]
To make this easier to understand, imagine that $\textit{confidence}(\abrule)=0.5$, and $\textit{support}(\setB)=0.4.$ This means that the presence of $\setA$ increases the probability of $\setB$ being in the same basket by $25\%$ ($\frac{0.5}{0.4}=1.25$), therefore providing us with a lift value of 1.25.  A lift value below 1 would indicate that the occurrence of $\setA$ in a basket decreases the likelihood of $\setB$ occurring in the same basket (i.e. a low product association).
\\
As established above,  the Apriori Principle states that all subsets of a frequent itemset must be frequent.  The Apriori Principle is a result of the \textit{anti-monotone property of support} \cite{anti_monotone}, which means for $\{A,B,C\}$, $\textit{support}(\{A,B\}) \geq \textit{support}(\{A,B,C\})$.  Therefore, the Apriori principle can be employed to prune the frequent itemsets much more efficiently, because if $support(\{A,B\}) < \textit{min}_{\textit{support}}$, then any itemset containing the set $\{A,B\}$ will also fall below $\textit{min}_{\textit{support}}$. Once the frequent itemsets have been pruned,  association rules can be generated from the remaining itemsets.  The resulting association rules can be even further pruned by removing those that fall below a confidence threshold $\textit{min}_{\textit{confidence}}$.  Finally, the remaining rules can be ranked according to their  lift to find the rules with the highest associations.  Figure \ref{fig:prune} is an illustration of how pruning one set $\{A, B\}$ leads to the pruning of all its children.

\begin{figure}[H]\centering
\includegraphics[scale=0.5]{apriori-prune}	
\caption{Pruning Infrequent Rules \cite{apriori-prune}}
\label{fig:prune}
\end{figure}

\subsection{Related Work}
\label{related-work}
\textbf{C. Zhong et al.} \cite{kmeans_mst} proposed a novel framework to extract the minimum spanning tree of a graph based on the K-Means clustering algorithm. Their methodology can be separated into two distinct phases. In the first phase,  the data is partitioned into $\sqrt{n}$ clusters via K-Means and the Kruskal's algorithm is applied to each of the clusters individually.  Once the $\sqrt{n}$ MSTs have been constructed, they are combined to form an approximate MST. In the second phase,  new partitions are constructed based on the borders of the clusters identified in phase 1.  Based on these new partitions, a second approximate MST is constructed.  Finally, both graphs are merged such that the resulting graph has $2(n-1)$ edges. The Kruskal's algorithm is run on this graph to get the final approximation of the MST.
\\\textbf{Critical Analysis}\\
The authors have proposed an efficient way to approximate an MST, with their methodology having a complexity of $O(N^{1.5})$, which is faster than the standard MST algorithm which has a complexity of $O(N^2)$. This may not intuitively seem like a large decrease, but as Figure \ref{fig:speed-compare} illustrates (plotted by the author), there is a significant increase in efficiency over the standard algorithm.
\begin{figure}[H]
\centering
\includegraphics[scale=0.6]{speed-compare} 
\caption{Efficiency of K-Means optimized MST vs.  exact MST}\label{fig:speed-compare}
\end{figure}
\noindent  The K-Means optimized algorithm is $\frac{\sqrt{N}-1}{\sqrt{N}}\%$  faster than the standard MST algorithm.
\\\\
\textbf{R. Aggarwal et al.} \cite{mine} proposed a novel algorithm to generate all statistically significant association rules between items in a database, laying the foundations for association rule mining.  Given a set of items $I = I_1, I_2, I_3,...I_m$, the authors define an association rule to be of the form $X \rightarrow I_j$ where X is a set of items such that $X \in I, I_j \notin X$.  The hypothetical database stated was a list of transactions, $T$, where each transaction $t$ was a binary vector of length $m$, representing a basket purchase, where $t[k] = 1$ if $I_k$ been purchased in that basket.  The authors stated that their methodology for association rule mining could be split into two steps: the generation of candidate itemsets, and the generation of statistically significant association rules from these itemsets. \\
To address the first subproblem, the authors provided the pseudo-code for their candidate itemset generation,  where all itemsets possible were generated from tuples (samples) from the database,  and those itemsets whose support score\footnote{see: Section \ref{mba_define}} is above the minimum support threshold are considered candidates (called \textit{large itemsets}).  Since a brute-force check would be sub-optimal  (the authors note this could take up to $2^m$ passes of the database),  the authors devised a methodology to check for candidate itemsets where on the $k^{th}$ pass of the database, they would only check itemsets of length $k$, to see if they satisfied the support constraint. On the $(k+1)^{th}$ pass, they need only check those itemsets that are 1-extensions (i.e.  itemsets extended by exactly one item) of the \textit{large itemsets} found in the previous pass. This is because of what would later be known as the \textit{Apriori Principle}, where if an itemset $Y$ is \textit{large}, then all subsets of $Y$ must also be \textit{large}. Therefore, if they found the itemset $\{A,B\}$ was \textit{small} (i.e. did not satisfy the support constraint), then sets containing $\{A,B\}$ (e.g. $\{A,B, C\}, \{A,B, D\}, \{A,B, C, D\}$) would also be small, and need not be checked. This means, however, that if an itemset $I$ is \textit{large}, then another pass over the dataset would be required to check the support of the subsets of $I$. To avoid this, the authors devised a measure to calculate the expected support, $\bar{s}$, of an itemset,  and use this to measure the support of itemsets $I = (X + I_j)$ not only where $I$ is expected to be large, but also where $X+I_j$ is expected to be small but $X$ is expected to be large, further helping them prune the number of itemsets to check.  The authors proceed to define a method that allows the algorithm to be more memory efficient\footnote{This may no longer be required due to the advances in computational power in the the 27 years since this paper was written.}. The authors also defined method to further prune itemsets from the search, namely \textit{remaining tuples optimization} and \textit{pruning function optimization}.\\
To address the second subprobem,  the authors proposed the following methodology: for each large itemset $Y = I_1, I_2,...I_k, k\geq 2$ from the set of non-pruned large itemsets, generate a set of association rules of form $X \rightarrow I_j$ such that the consequent is $I_j$ and the antecedent (i.e. the precedent set in the rule) is a subset $X$ of $Y$ such that $X$ is of length $k-1$ and $I_j \notin X$. Therefore, each large itemset will produce $k$ rules. From the generated rules, the authors discarded those rules whose confidence scores\footnote{see: Section \ref{mba_define}} fell below the minimum confidence threshold $c$.\\
The authors tested their methodology on a sales dataset with $46,783$ transactions,  with $63$ \textit{items} (in this case, the department from which the customer bought an item). They used a configuration of a minimum support threshold of $1\%$ and a minimum confidence threshold of $50\%$. The rules produced seemed to follow with general intuition, such as:\\
$\{\text{Auto Accessories, Tires}\} \rightarrow \{\text{Automotive Services}\} $\\
Furthermore, the authors assessed the accuracy of their expectation method, by measuring the ratio of correctly estimated itemsets for both small and large, against various values for the minimum support threshold, and visualizing the result. To isolate the effect of their expectation method, they disabled their pruning optimization functions. They were able to conclude that their estimation accuracy was satisfactory, as their accuracy was above 98\% for support thresholds except the first, where it was 96\%.  The authors also tested the effectiveness of their pruning optimization functions, namely the \textit{remaining tuples} and the \textit{pruning function} optimization functions, against multiple minimum support threshold values. They were able to conclude that their pruning efficiency increased as the support threshold increased.
\\\textbf{Critical Analysis}\\
The authors have proposed a novel methodology that has been the bedrock of numerous research publications,  including most of the papers in this literature review.  Their estimation function performed with high accuracy, meaning it can reduce the number of passes through a database the algorithm has to take by a significant amount. Additionally, their pruning techniques allowed them to eliminate a large proportion of itemsets from the space. Even after the significant pruning of rules, a major drawback of this methodology is the large number of rules produced, although one could argue that only the highest performing rules need be observed in further detail.
\\\\
\textbf{M. Zekić-Sušac et al.} \cite{market_ass} proposed a novel measure for the \textit{interestingness} of association rules,  identifying that a dominant, universally used measure did not exist. The authors' goal was to combine objective measures such as the support, confidence and lift scores with more subjective measures. Instead of the Apriori approach, their methodology has them generate association rules via the \textit{tree-building technique} - which compresses a large database into a Frequent-Pattern tree, citing that this technique was more efficient than the Apriori algorithm. The author's employed the heuristical unexpectedness measure(i.e.  significantly contradicting a user's prior beliefs) and the heuristical actionability measure (i.e. if the end user can use the information to their advantage(e.g. a promotion)) as their subjective measures, and a minimum confidence threshold of 51\% as their subjective measure.  Since a subjective measure would require a human subject, the authors' used the estimations of a sales manager from a Croatian retail chain, and stored his responses in binary format for the subjective measures (i.e. 0 if a rule was unexpected else 1, 0 if a rule is not useful else 1). The dataset used for this paper was a real transactional dataset with 14,012 transactions and a set of 1,230 unique items (which was later pruned to 7,006 transactions and a set of 278 products) from the same Croatian retail chain their test subject worked at. The authors then generated association rules from the first-level hierarchical grouping of items from the dataset (items with a minimum support of 25\%),  of which 36 rules were identified as statistically significant.  From this set of rules, only two rules satisfied both subjective measures and the confidence constraint, and therefore these two rules were identified as highly interesting.  The authors then generated association rules from the second-level hierarchical grouping of items, where items that represented the same product (but had different a manufacturer, brand etc.) were grouped together. Of the rules generated, 15 satisfied their confidence constraints and had a support value able 10\%. 5 rules from this set satisfied both their subjective and objective measures,  more than the previous experiment  The authors were able to conclude that the increase in accuracy and number of interesting rules resulted from the second level of grouping which generalized the products.\\
\textbf{Critical Analysis}\\
Wheras the original measure of statistical significance introduced by R. Aggarwal et al.  was purely objective, the authors  of this paper have presented a well thought out approach to combining the subjective metrics with objective ones to produce a human-verified association rule set.  A few caveats to note, however: their study only involved one subject, which is rarely regarded to be statistically acceptable. An ideal study would require multiple, randomly chosen subjects to offset any bias that the singular subject would have had, and in addition, the larger their subject size, the closer their collective estimations will model the total population's.  Another drawback of their approach is that by using human intuition as a metric,  they're promoting association rules that satisfy pre-existing notions about human behavior (e.g. if someone buys milk, they'll \textit{probably} get eggs too), however these types of rules are usually regarded as common knowledge,  whereas the beauty of association rule mining is in its ability to surface association rules that - while true - seem unintuitive, and therefore are less likely to be known by the management of such organizations.
\\\\
% MAIN MST PAPER
\textbf{M. A. Valle et al.} \cite{mst_paper} proposed a novel methodology to study the structure and behavior of consumer market baskets from the topology of a minimum spanning tree which represented the interdependencies between products, and use this information to complement the association rule generation process. The input to their proposed methodology was a correlation matrix between the set of all one-hot encoded purchase vectors such that each vector denoted the presence or non-presence of each product from the dataset in that vector.  The dataset used for the MST construction was a list of $1,046,804$ transactions containing a set of $3,240$ unique products from a large supermarket chain branch in Santiago, Chile.  When building this correlation matrix, the authors opted to use the Pearson's Coefficient (which is equivalent to the coefficient $\phi$ for binary data such as theirs) over the traditionally used Jaccard distance to compute the similarity between the binary product vectors, as the former provides both a positive and negative association between products. Additionally,  they used the distance function $d_{ij} = \sqrt{2(1-\phi_{ij}}$ to transform the correlation matrix into a distance measurement (i.e. the weight of the edges).  The authors constructed a MST for 220 product subcategories, and noted that there was a significant level of grouping between product sub-categories that belonged to the same parent category.  To remove edges from the MST that were not statistically significant,  the authors used the mutual information \cite{measure} measure $\sum\limits_{x,y}log_2 \frac{r(x,y)}{p(x)q(y)}$ between product subcategories $p$ and $q$, and were able to prune 14 edges, all of which were connected to a terminal node, therefore effectively pruning 14 vertices from the MST too. To identify the most influential regions of the MST, the authors defined an influence zone of distances that were in the $10^{th}$ percentile. To generate meaningful association rules,  for each MST product $i$, the authors ran a search for the set of all association rules $R_i$ such that $P_i \rightarrow P_j (i \neq q)$. Then from the resulting set of rules, they searched for rules that obeyed $P_i \rightarrow P_m$ where $m$ a product node connected to the product $i$ in the minimum spanning tree.  For both resulting sets of rules for each product, the mean of their lift scores were observed, and the authors determined that the rules that were reinforced by the MST had a higher mean, and that a majority of these rules had a lift score above the $90^{th}$ percentile. 
\\
To identify the clusters each of the products should be identified under, the authors constructed a hierarchical tree using the average linkage clustering method, and by using an unspecified cut distance, they were able to produce 17 taxonomic groups (i.e. clusters). Cross-referencing their results with the actual parent categories of the products, they were able to conclude that the MST did indeed categorize the product sub-categories into clusters with a reasonable degree of accuracy. The authors then compared their MST to another methodology, namely the structured association map (SAM) \cite{kim}, using the Jaccard distance as a measure of similarity,  and were able to generate interesting 2x2 rules (i.e. $\{A,B\}\rightarrow\{C, D\}$), all with lift scores above 1.0, with one rule even having a lift score of 106.46. They concluded that while both approaches provided different information, they both visually identify the strongest relationships between the products, and provide useful information to reduce the search space for association rules.
\\\textbf{Critical Analysis}\\
The authors' approach seems to be novel,  thorough and well structured.  Their methodology successfully employed the use of minimum spanning trees to complement the association rule generation process with sound results.  One caveat of their approach is that they only used the MST to generate 1x1 rules (i.e. $\abrule$). Using the distance score in conjunction with the importance function they defined (i.e. $\sum\limits_{k \in K_u}^{} \frac{1}{w_{uk}}$), they could have defined a system to produce $p \times q$ rules (where $ p \geq 1, q \geq 1$;i.e. rules that have one \textit{or more} items in the antecedent and consequent), then rank them using their respective lifts.  Additionally, while the authors did cross reference their clustering results against the real parent categories of the products, they did not compare their results to that of a clustering algorithm (e.g. K-Means), which would have given a reasonable benchmark to compare the results of the MST clustering to. While 1x1 rules are easily understandable and tend to have high lift values when extracted from the MST, $p \times q$ rules would provide a layer of insight as to how a range of products (perhaps a cluster) related to another.
\\\\
\textbf{Summary}\\
In conclusion, \cite{mine} introduced a formal system for association rules, and an to generate them from a transactional database - the Apriori Algorithm \cite{apriori}.  However, while these papers only based the interestingness of an association rule on objective measures such as the support, confidence and lift scores of these rules, \cite{market_ass} proposed a methodology where subjective human input was used to validate the interestingness of these rules. A drawback of the above methods, however, is the large number of rules produced. The paper \cite{mst_paper} describes a methodology to extract high value association rules from a minimum spanning tree, used to complement the rules produced by the Apriori algorithm. A caveat of this approach, however, is that it can only produce rules such that the antecedent and consequent are sets with one element each. Additionally, the methodology described involves building the MST using Prim's algorithm, which can be relatively slow. \cite{kmeans_mst} proposed a solution to this, where the Kruskal's algorithm was optimized using K-Means, leading to a significant performance increase as illustrated in Figure \ref{fig:speed-compare}. The methodology proposed in this paper was a framework of sorts, where Kruskal's could be substituted with any MST algorithm, such as Prim's.\\
All the research conducted above has served as the inspiration for this project.
\newpage
\section{Methodology \& Requirement Analysis}
\subsection{Requirements Analysis}
This section will identify both the functional and non-functional requirements required for the implementation of the model proposed in this document. By stating the specific requirements of the model,  it serves as a high-level view of how the model should function. 
 Using the MoSCoW system, each requirement has been given one of the following priorities: must have, should have, could have and would have.
 % FUNCTIONAL
\begin{table}[H]
\centering
\caption{Functional Requirements}
\begin{tabular}{@{}rlll@{}}
\toprule
\textbf{ID} & \textbf{Category} & \textbf{Priority} & \textbf{Description}                                                  \\ \midrule
FR1         & Dataset           & Must Have         & The dataset must be void of empty values.                             \\
FR2         & Dataset           & Must Have         & The dataset must have a unique identifier for each transaction.       \\
FR3         & Model             & Must Have         & A list of binary purchase vectors must be extracted from the dataset. \\
FR4         & Model             & Must Have         & The model must return a storable data type representation of a MST.   \\
FR5         & Model             & Should Have       & The model must produce  $p \times q$ association rules.               \\ \bottomrule
\end{tabular}
\label{tab:fr}
\end{table}

% NON-FUNCTIONAL
\begin{table}[H]
\centering
\caption{Non-Functional Requirements}
\begin{tabular}{@{}rllp{0.7\textwidth}@{}}
\toprule
\textbf{ID} & \textbf{Category} & \textbf{Category} & \textbf{Description}                                                                                                                          \\ \midrule
NFR1        & Dataset           & Should Have       & The dataset will contain at least 500,000 unique transactions.                                                                                \\
NFR2        & Hardware          & Could Have        & The data inputs and the MST construction will run on a computer with at least 8 GB of ram, and a quad-core CPU with clock speed above 2.5GHz. \\ \bottomrule
\end{tabular}
\label{nfr}
\end{table}

\subsection{Research Methodology}
\subsubsection{Technical Choices}
The model will be built using Python 3.8 \cite{py}. It is a versatile, fast language with both object-oriented and functional paradigms, with vast support and numerous libraries for data analytics and machine learning, many of which act as an interface for functions written in faster languages such as C/C++. The following libraries will be used to help optimize the development process:
\begin{itemize}
\item pandas
\item numpy
\item scikit-learn
\item matplotlib
\item graphviz
\item efficient-apriori
\end{itemize}
\subsubsection{Implementation}
The primary objective of this project is to analyze the viability of a framework based on the minimum spanning tree as a 'one-in-all' tool for market basket analysis,  from association rule generation to clustering.  This section will describe how the author plan to achieve these goals:\\
A candidate dataset has been identified: a dataset of $~27,000,000$ transactions occurring across multiple branches of a Brazilian gas station store \cite{data_source}. Each row in the dataset contains the purchase information for a particular product for a given transaction. The dataset has been checked for errors and inconsistencies, and any found have been remedied. Additionally, since the transaction id is an unreliable identifier for a market basket\footnote{transaction numbers reset once they have reached an upper limit, transaction ids are not synced across stores so the same transaction id could refer to multiple transactions across different stores.},  each row has been assigned a unique alphanumeric code consisting of the purchase's transaction number, city and date, which will act as a robust unique identifier for each basket. F Additionally, redundant or unnecessary features have been removed from the dataset, this will allow us to make quicker passes through all the data when generating the binary purchase vectors.  Once the dataset is prepared, the binary purchase matrix will need to be generated from the dataset.  Since each product purchase has an assigned transaction number, one pass through the database will be needed to find all product purchases in a transaction. Therefore, to build $n$ binary purchase vectors, $n$ passes of the database will be required (i.e.  a complexity of $O(n^2)$). This will prove to be quite computationally expensive,  as there are approximately $5,200,000$ unique baskets in the dataset.  Even when only considering $473,641$ baskets with $40$ unique products, the resulting matrix (see Fig. \ref{fig:corrmat}) took over 6 hours to construct.  The author will be looking into more efficient ways to generate out input data. \\
\begin{figure}[H]
\centering
\includegraphics[scale=0.45]{corrmat}
\caption{Correlation Matrix for 473,641 baskets}
\label{fig:corrmat}
\end{figure}
Once the correlation matrix has been generated and a distance function applied, a graph will have to be constructed.  Given the size of the dataset, extracting a MST from the graph using traditional algorithms such as Prim's and Kruskal's may be ineffective, and more recent approaches may need to be considered. For example,  \textbf{C. Zhong et al.} \cite{kmeans_mst} proposed a divide-and-conquer algorithm for extracting a minimum spanning tree based on the K-Means clustering algorithm, which may be worth considering over Prim's and Kruskal's.  Using the appropriate extraction method,  the MST will be extracted from the graph.  Following this, the above methodology will be repeated after aggregating the products (e.g.  renaming all the brands of lubricant to 'lubricant', similar to the second-level hierarchical grouping in \cite{market_ass}) ,  which will allow the author to study how these generalized products affect the structure of the MST. From the resulting minimum spanning trees,  the most interesting association rules (i.e. those with the highest statistical significance) can be extracted.  This process will be repeated several times,  limiting the dataset to a particular city each iteration to observe how the architecture of the MST differs by city as compared to the dataset as a whole.

\newpage
\section{Evaluation Strategy}
The model will need to evaluated to ensure that:
\begin{enumerate}
\item The model satisfies all the listed functional requirements listed in Table \ref{tab:fr}.
\item The MST must be able to produce high value association rules.
\item The MST architecture must indicate a level of product grouping that is consistent with their parent subcategories.
\end{enumerate}

\subsubsection*{Functional Requirement Validation}
\texttt{FR1: The dataset must be void of empty values.}\\
To evaluate this,  the database will be traversed, checking for empty/null values. The result of this evaluation is binary - no empty values found or more than 0 found.\\\\
\texttt{FR2: The dataset must have a unique identifier for each transaction.}\\
To evaluate this, a script will be written to validate if each product purchase's associated transaction identifier matches the expected value. The result of this evaluation is also binary - either all unique transaction identifiers are as expected, or otherwise.\\\\
\texttt{FR3: A list of binary purchase vectors must be extracted from the dataset.}\\
Using the unique transaction identifier for each product in the database, the binary purchase vectors can be generated.  To evaluate this,  the dimensions matrix of binary purchase vectors will be checked to ensure that they are consistent with the expected value.
\texttt{FR4: The model must return a storable data type representation of a MST.}\\
The model will be stored as a matrix datatype, and will be evaluated by its ability to successfully be exported as a serialized file.\\\\
\texttt{FR5: The model should produce $p \times q$ rules.}\\
After the association rules have been generated, this requirement will be evaluated by searching the association rules to ensure at least one rule satisfies this requirement.

\subsubsection*{Rule "Interestingness" Validation}
There are two ways to validate the \textit{interestingness} of an association rule:
\begin{itemize}
\item Objective measures: Metrics such as the support, confidence and lift scores will be used to rank the rule and evaluate its relative position against all the other rules.  Additionally, a greater number of rules will be generated using the Apriori algorithm,  and the rule's relative position will be compared to these rules using the aforementioned metrics.
\item Subjective measures: Due to the ongoing pandemic, the author will not be pursuing subjective measures to validate our rules, as subjective evaluation will require a third party to be present.
\end{itemize}

\subsubsection*{MST Grouping Validation}
One way to validate the MST is to check the grouping of products in the MST. Ideally,  grouping of similar products should be present (e.g. strong grouping between dairy products). A threshold can be set on the edge distance, such that any products within that threshold radius can be considered a cluster.  One objective way to evaluate this cluster is to build a confusion matrix with the products against their parent categories and evaluate the proportion of products grouped together under their correct parent category. It would also be interesting to compare the results of the MST grouping to the clusters generated by an unsupervised clustering algorithm such as the K-Means clustering algorithm.

\newpage
\section{Project Management}
\label{sec:mgmt}
To ensure that the project is of optimal quality,  a plan must be charted out that specifies a timetable and deadlines for milestones in the project.
\subsection{Methodology}
There are several formal approaches for development planning,  but the most optimal process for us is the Waterfall method \cite{waterfall}, as the methodology can be defined as a linear, sequential workflow, where the requirements have been explicitly stated and change is unlikely.
\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{waterfall}
\caption{An outline of the waterfall method}\label{fig:waterfall}
\end{figure}

\subsection{Gantt Chart}
The Gantt charts are a visualization of the milestone deadlines and allocated working periods.

\begin{figure}[H]
\centering
\includegraphics[scale=0.25]{gant1}
\caption{Gantt chart for Semester 1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{gant2}
\caption{Gantt chart for Semester 2}
\end{figure}

\subsection{Risk Analysis}
The probabilities of the risks can be discretized and approximated to the following values:
\begin{table}[H]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{9AFF99}}l |
>{\columncolor[HTML]{FFFFC7}}l |
>{\columncolor[HTML]{F8FF00}}l |
>{\columncolor[HTML]{FFCB2F}}l |
>{\columncolor[HTML]{FF6F6F}}l |}
\hline
Very Low & Low & Moderate & High & Very High \\ \hline
\end{tabular}
\end{table}
\noindent The impact of a given risk can be defined as follows:
\begin{table}[H]
\centering
\begin{tabular}{|
>{\columncolor[HTML]{FFFFC7}}l |
>{\columncolor[HTML]{F8FF00}}l |
>{\columncolor[HTML]{FF6F6F}}l |}
\hline
Tolerable & Serious & Catastrophic \\ \hline
\end{tabular}
\end{table}
\noindent Given these metrics,  the risks are outlined as follows:
\begin{table}[H]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\multicolumn{5}{c}{Risks}                                                          \\ \midrule
ID & Risk                                            & Probability & Impact       & Priority \\ \midrule
R1 & Data loss/corruption                            & Low         & Catastrophic & High     \\
R2 & Code loss/corruption                            & Low         & Catastrophic & High     \\
R3 & Inability to meet deadlines                     & Low         & Serious      & Medium   \\
R4 & Incorrect estimation of model development time & Low         & Serious      & Low      \\
R5 & Unrealistic computation times                   & High        & Catastrophic & High     \\ \bottomrule
\end{tabular}
\end{table}
\noindent The table above lists the risks identified with their probabilities, their impact, and the priority given to them. Each risk has given an alphanumeric ID to cross reference it against the table below, which describes the risk management for each of the risks identified. 

\begin{longtable}[c]{|p{0.05\textwidth}|p{0.31667\textwidth}|p{0.31667\textwidth}|p{0.31667\textwidth}|}
\hline
\multicolumn{4}{|c|}{Risk Management}                                                                                                                                                                                                                        \\ \hline

ID & Avoidance                                                                          & Minimization                                                                           & Contingency                                                               \\ \midrule
\endhead
R1 & Store backups of datasets locally and on cloud storage at frequent intervals.      & Do not overwrite existing datasets.                                                    & Retrieve the last checkpoint data.                                        \\ \midrule
R2 & Maintain a git repository with two online remotes, with frequent commits.            & Only commit when the code is known to work correctly.                                  & Roll-back to a previous, working commit.                                  \\ \midrule
R3 & Set milestone deadlines in the project plan.                                       & Collaborate with supervisor to come up with a robust yet flexible project plan.        & Restructure plan to offset any delays.                                    \\ \midrule
R4 & Create realistic plan with reasonable tolerances to account for unexpected delays. & Attempt to finish all tasks within their appropriate period.                           & Restructure plan with updated tolerances.                                 \\ \midrule
R5 & Perform computations on a small subset of the dataset.                             & Prototype model with very small dataset subsets and increase once the model is robust. & Reduce the dataset size until the computation is affordable.              \\ \bottomrule
\end{longtable}

\subsection{Professional, Ethical, Legal \& Social Issues}
\subsubsection*{Professional Issues}
To ensure a level of professionalism is always upheld, the author has identified key risks and noted their contingencies. In addition, the author has developed - in collaboration with the supervisor - a project plan that ensures that all parties involved are suitably informed about development milestones and their deadlines.
\subsubsection*{Legal Issues}
The only software being used for the development of this project is Jupyter Notebook - an open source web-based development tool that is licensed under a modified BSD license. The dataset being used for this project has been anonymized, with the names of the products, customers and employees being modified to avoid identification. Therefore it can be concluded that there are no legal issues present.
\subsubsection*{Ethical Issues}
Since this project does not involve the participation of any human subjects, there are no ethical issues present.
\subsubsection*{Social Issues}
There are no social issues in this project.


% Keep references on new page
\newpage
\printbibliography

\end{document}
